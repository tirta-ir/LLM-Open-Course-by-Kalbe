{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["wP01iHgmZMd7","uyOtCgFCZNj2","qOJApRifZOZd"],"authorship_tag":"ABX9TyPpUwNsHXA6niPMPjUS/rjx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wP01iHgmZMd7"},"source":["# What to do when you get an error"]},{"cell_type":"markdown","metadata":{"id":"eOG2t-d2ZMd9"},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pD4foWM9ZMd-"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]\n","!apt install git-lfs"]},{"cell_type":"markdown","metadata":{"id":"Ri0RWNMzZMd_"},"source":["You will need to setup git, adapt your email and name in the following cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ir5xagfAZMd_"},"outputs":[],"source":["!git config --global user.email \"you@example.com\"\n","!git config --global user.name \"Your Name\""]},{"cell_type":"markdown","metadata":{"id":"ey8eO0ScZMd_"},"source":["You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RYo9WJtNZMd_"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-c3kjD3ZMeA"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MxWEy4yOZMeA"},"outputs":[],"source":["from distutils.dir_util import copy_tree\n","from huggingface_hub import Repository, snapshot_download, create_repo, get_full_repo_name\n","\n","\n","def copy_repository_template():\n","    # Clone the repo and extract the local path\n","    template_repo_id = \"lewtun/distilbert-base-uncased-finetuned-squad-d5716d28\"\n","    commit_hash = \"be3eaffc28669d7932492681cd5f3e8905e358b4\"\n","    template_repo_dir = snapshot_download(template_repo_id, revision=commit_hash)\n","    # Create an empty repo on the Hub\n","    model_name = template_repo_id.split(\"/\")[1]\n","    create_repo(model_name, exist_ok=True)\n","    # Clone the empty repo\n","    new_repo_id = get_full_repo_name(model_name)\n","    new_repo_dir = model_name\n","    repo = Repository(local_dir=new_repo_dir, clone_from=new_repo_id)\n","    # Copy files\n","    copy_tree(template_repo_dir, new_repo_dir)\n","    # Push to Hub\n","    repo.push_to_hub()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8bvGOAH5ZMeA","outputId":"2af21f1e-8a9f-47a7-dac2-36a3b1fc44aa"},"outputs":[{"data":{"text/plain":["\"\"\"\n","OSError: Can't load config for 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:\n","\n","- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'\n","\n","- or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file\n","\"\"\""]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import pipeline\n","\n","model_checkpoint = get_full_repo_name(\"distillbert-base-uncased-finetuned-squad-d5716d28\")\n","reader = pipeline(\"question-answering\", model=model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sAW9pqorZMeB","outputId":"6b81eb74-9ec6-45d9-d194-9583a134cc4e"},"outputs":[{"data":{"text/plain":["\"\"\"\n","OSError: Can't load config for 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:\n","\n","- 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'\n","\n","- or 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file\n","\"\"\""]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["model_checkpoint = get_full_repo_name(\"distilbert-base-uncased-finetuned-squad-d5716d28\")\n","reader = pipeline(\"question-answering\", model=model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qneDRfk7ZMeB","outputId":"21e467d4-ef24-4060-ba13-341cfcdf722e"},"outputs":[{"data":{"text/plain":["['.gitattributes', 'README.md', 'pytorch_model.bin', 'special_tokens_map.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.txt']"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from huggingface_hub import list_repo_files\n","\n","list_repo_files(repo_id=model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iIDWaM4YZMeB"},"outputs":[],"source":["from transformers import AutoConfig\n","\n","pretrained_checkpoint = \"distilbert-base-uncased\"\n","config = AutoConfig.from_pretrained(pretrained_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"88lwfHsSZMeB"},"outputs":[],"source":["config.push_to_hub(model_checkpoint, commit_message=\"Add config.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rgu5UyfmZMeB","outputId":"f956d7a6-b9ba-43df-f058-25f8a19ba6b6"},"outputs":[{"data":{"text/plain":["{'score': 0.38669535517692566,\n"," 'start': 34,\n"," 'end': 95,\n"," 'answer': 'the task of extracting an answer from a text given a question'}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["reader = pipeline(\"question-answering\", model=model_checkpoint, revision=\"main\")\n","\n","context = r\"\"\"\n","Extractive Question Answering is the task of extracting an answer from a text\n","given a question. An example of a question answering dataset is the SQuAD\n","dataset, which is entirely based on that task. If you would like to fine-tune a\n","model on a SQuAD task, you may leverage the\n","examples/pytorch/question-answering/run_squad.py script.\n","\n","ü§ó Transformers is interoperable with the PyTorch, TensorFlow, and JAX\n","frameworks, so you can use your favourite tools for a wide variety of tasks!\n","\"\"\"\n","\n","question = \"What is extractive question answering?\"\n","reader(question=question, context=context)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"trC8IOKUZMeC"},"outputs":[],"source":["tokenizer = reader.tokenizer\n","model = reader.model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dmEL_2huZMeC"},"outputs":[],"source":["question = \"Which frameworks can I use?\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_2Z_9TRKZMeC","outputId":"904bd4be-7d82-4042-d711-4ceb81aa999d"},"outputs":[{"data":{"text/plain":["\"\"\"\n","---------------------------------------------------------------------------\n","AttributeError                            Traceback (most recent call last)\n","/var/folders/28/k4cy5q7s2hs92xq7_h89_vgm0000gn/T/ipykernel_75743/2725838073.py in <module>\n","      1 inputs = tokenizer(question, text, add_special_tokens=True)\n","      2 input_ids = inputs[\"input_ids\"]\n","----> 3 outputs = model(**inputs)\n","      4 answer_start_scores = outputs.start_logits\n","      5 answer_end_scores = outputs.end_logits\n","\n","~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n","   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n","   1050                 or _global_forward_hooks or _global_forward_pre_hooks):\n","-> 1051             return forward_call(*input, **kwargs)\n","   1052         # Do not call functions when jit is used\n","   1053         full_backward_hooks, non_full_backward_hooks = [], []\n","\n","~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\n","    723         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","    724\n","--> 725         distilbert_output = self.distilbert(\n","    726             input_ids=input_ids,\n","    727             attention_mask=attention_mask,\n","\n","~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n","   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n","   1050                 or _global_forward_hooks or _global_forward_pre_hooks):\n","-> 1051             return forward_call(*input, **kwargs)\n","   1052         # Do not call functions when jit is used\n","   1053         full_backward_hooks, non_full_backward_hooks = [], []\n","\n","~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\n","    471             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n","    472         elif input_ids is not None:\n","--> 473             input_shape = input_ids.size()\n","    474         elif inputs_embeds is not None:\n","    475             input_shape = inputs_embeds.size()[:-1]\n","\n","AttributeError: 'list' object has no attribute 'size'\n","\"\"\""]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","\n","inputs = tokenizer(question, context, add_special_tokens=True)\n","input_ids = inputs[\"input_ids\"][0]\n","outputs = model(**inputs)\n","answer_start_scores = outputs.start_logits\n","answer_end_scores = outputs.end_logits\n","# Get the most likely beginning of answer with the argmax of the score\n","answer_start = torch.argmax(answer_start_scores)\n","# Get the most likely end of answer with the argmax of the score\n","answer_end = torch.argmax(answer_end_scores) + 1\n","answer = tokenizer.convert_tokens_to_string(\n","    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",")\n","print(f\"Question: {question}\")\n","print(f\"Answer: {answer}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YYPhdgM7ZMeC","outputId":"1b807a56-27a1-4645-d0eb-056e62fb9161"},"outputs":[{"data":{"text/plain":["[101, 2029, 7705, 2015, 2064]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["inputs[\"input_ids\"][:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cG7VcBK2ZMeC","outputId":"90d4a58e-51d4-40f1-ea43-311bfa3f914a"},"outputs":[{"data":{"text/plain":["list"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["type(inputs[\"input_ids\"])"]},{"cell_type":"markdown","source":[],"metadata":{"id":"B4cOChJ_ZZzT"}},{"cell_type":"markdown","metadata":{"id":"uyOtCgFCZNj2"},"source":["# Asking for help on the forums"]},{"cell_type":"markdown","metadata":{"id":"0L5grhTEZNj6"},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QuU6M8cXZNj7"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5eJc8cdtZNj8"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModel\n","\n","model_checkpoint = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","model = AutoModel.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fa4sg21aZNj8"},"outputs":[],"source":["text = \"\"\"\n","Generation One is a retroactive term for the Transformers characters that\n","appeared between 1984 and 1993. The Transformers began with the 1980s Japanese\n","toy lines Micro Change and Diaclone. They presented robots able to transform\n","into everyday vehicles, electronic items or weapons. Hasbro bought the Micro\n","Change and Diaclone toys, and partnered with Takara. Marvel Comics was hired by\n","Hasbro to create the backstory; editor-in-chief Jim Shooter wrote an overall\n","story, and gave the task of creating the characthers to writer Dennis O'Neil.\n","Unhappy with O'Neil's work (although O'Neil created the name \"Optimus Prime\"),\n","Shooter chose Bob Budiansky to create the characters.\n","\n","The Transformers mecha were largely designed by Sh≈çji Kawamori, the creator of\n","the Japanese mecha anime franchise Macross (which was adapted into the Robotech\n","franchise in North America). Kawamori came up with the idea of transforming\n","mechs while working on the Diaclone and Macross franchises in the early 1980s\n","(such as the VF-1 Valkyrie in Macross and Robotech), with his Diaclone mechs\n","later providing the basis for Transformers.\n","\n","The primary concept of Generation One is that the heroic Optimus Prime, the\n","villainous Megatron, and their finest soldiers crash land on pre-historic Earth\n","in the Ark and the Nemesis before awakening in 1985, Cybertron hurtling through\n","the Neutral zone as an effect of the war. The Marvel comic was originally part\n","of the main Marvel Universe, with appearances from Spider-Man and Nick Fury,\n","plus some cameos, as well as a visit to the Savage Land.\n","\n","The Transformers TV series began around the same time. Produced by Sunbow\n","Productions and Marvel Productions, later Hasbro Productions, from the start it\n","contradicted Budiansky's backstories. The TV series shows the Autobots looking\n","for new energy sources, and crash landing as the Decepticons attack. Marvel\n","interpreted the Autobots as destroying a rogue asteroid approaching Cybertron.\n","Shockwave is loyal to Megatron in the TV series, keeping Cybertron in a\n","stalemate during his absence, but in the comic book he attempts to take command\n","of the Decepticons. The TV series would also differ wildly from the origins\n","Budiansky had created for the Dinobots, the Decepticon turned Autobot Jetfire\n","(known as Skyfire on TV), the Constructicons (who combine to form\n","Devastator),[19][20] and Omega Supreme. The Marvel comic establishes early on\n","that Prime wields the Creation Matrix, which gives life to machines. In the\n","second season, the two-part episode The Key to Vector Sigma introduced the\n","ancient Vector Sigma computer, which served the same original purpose as the\n","Creation Matrix (giving life to Transformers), and its guardian Alpha Trion.\n","\"\"\"\n","\n","inputs = tokenizer(text, return_tensors=\"pt\")\n","logits = model(**inputs).logits"]},{"cell_type":"markdown","source":[],"metadata":{"id":"CvjN1C1KZcdj"}},{"cell_type":"markdown","metadata":{"id":"qOJApRifZOZd"},"source":["# Debugging the training pipeline"]},{"cell_type":"markdown","metadata":{"id":"Nr42S9iqZOZg"},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ePOMp4EZOZg"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pXZdF2_AZOZh","outputId":"7eda2f91-b5f3-4581-ec17-554d9375de49"},"outputs":[{"data":{"text/plain":["ValueError: No gradients provided for any variable: ['tf_distil_bert_for_sequence_classification/distilbert/embeddings/word_embeddings/weight:0', '...']"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset\n","import evaluate\n","from transformers import (\n","    AutoTokenizer,\n","    TFAutoModelForSequenceClassification,\n",")\n","\n","raw_datasets = load_dataset(\"glue\", \"mnli\")\n","\n","model_checkpoint = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","\n","def preprocess_function(examples):\n","    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n","\n","\n","tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n","\n","train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n","    columns=[\"input_ids\", \"labels\"], batch_size=16, shuffle=True\n",")\n","\n","validation_dataset = tokenized_datasets[\"validation_matched\"].to_tf_dataset(\n","    columns=[\"input_ids\", \"labels\"], batch_size=16, shuffle=True\n",")\n","\n","model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n","\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n","\n","model.fit(train_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K6_2SZZeZOZi","outputId":"f4bf5fc0-d4fa-486b-e0ed-6793ccb8ec55"},"outputs":[{"data":{"text/plain":["{'attention_mask': <tf.Tensor: shape=(16, 76), dtype=int64, numpy=\n"," array([[1, 1, 1, ..., 0, 0, 0],\n","        [1, 1, 1, ..., 0, 0, 0],\n","        [1, 1, 1, ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1, ..., 1, 1, 1],\n","        [1, 1, 1, ..., 0, 0, 0],\n","        [1, 1, 1, ..., 0, 0, 0]])>,\n"," 'label': <tf.Tensor: shape=(16,), dtype=int64, numpy=array([0, 2, 1, 2, 1, 1, 2, 0, 0, 0, 1, 0, 1, 2, 2, 1])>,\n"," 'input_ids': <tf.Tensor: shape=(16, 76), dtype=int64, numpy=\n"," array([[ 101, 2174, 1010, ...,    0,    0,    0],\n","        [ 101, 3174, 2420, ...,    0,    0,    0],\n","        [ 101, 2044, 2048, ...,    0,    0,    0],\n","        ...,\n","        [ 101, 3398, 3398, ..., 2051, 2894,  102],\n","        [ 101, 1996, 4124, ...,    0,    0,    0],\n","        [ 101, 1999, 2070, ...,    0,    0,    0]])>}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["for batch in train_dataset:\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gKX0ZV3GZOZi","outputId":"f9efb21b-ed93-4244-9f98-0f8b885f7c38"},"outputs":[{"data":{"text/plain":["  246/24543 [..............................] - ETA: 15:52 - loss: nan"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["model.compile(optimizer=\"adam\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z7fFmN5zZOZi","outputId":"71006fe9-cbac-445e-d253-16f20c417bfd"},"outputs":[{"data":{"text/plain":["TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(16,), dtype=float32, numpy=\n","array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n","       nan, nan, nan], dtype=float32)>, logits=<tf.Tensor: shape=(16, 2), dtype=float32, numpy=\n","array([[nan, nan],\n","       [nan, nan],\n","       [nan, nan],\n","       [nan, nan],\n","       [nan, nan],\n","       [nan, nan],\n","       [nan, nan],\n","       [nan, nan],\n","       [nan, nan],\n","       [nan, nan],\n","       [nan, nan],\n","       [nan, nan],\n","       [nan, nan],\n","       [nan, nan],\n","       [nan, nan],\n","       [nan, nan]], dtype=float32)>, hidden_states=None, attentions=None)"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["model(batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zcstr6lcZOZj","outputId":"40cc731f-1895-41d8-bbe1-de3e38430f33"},"outputs":[{"data":{"text/plain":["TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(16,), dtype=float32, numpy=\n","array([0.6844486 ,        nan,        nan, 0.67127866, 0.7068601 ,\n","              nan, 0.69309855,        nan, 0.65531296,        nan,\n","              nan,        nan, 0.675402  ,        nan,        nan,\n","       0.69831556], dtype=float32)>, logits=<tf.Tensor: shape=(16, 2), dtype=float32, numpy=\n","array([[-0.04761693, -0.06509043],\n","       [-0.0481936 , -0.04556257],\n","       [-0.0040929 , -0.05848458],\n","       [-0.02417453, -0.0684005 ],\n","       [-0.02517801, -0.05241832],\n","       [-0.04514256, -0.0757378 ],\n","       [-0.02656011, -0.02646275],\n","       [ 0.00766164, -0.04350497],\n","       [ 0.02060014, -0.05655622],\n","       [-0.02615328, -0.0447021 ],\n","       [-0.05119278, -0.06928903],\n","       [-0.02859691, -0.04879177],\n","       [-0.02210129, -0.05791225],\n","       [-0.02363213, -0.05962167],\n","       [-0.05352269, -0.0481673 ],\n","       [-0.08141848, -0.07110836]], dtype=float32)>, hidden_states=None, attentions=None)"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n","model(batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oP5mfUgQZOZj","outputId":"6fdff5ca-ce14-46e1-e994-ad5cf35da427"},"outputs":[{"data":{"text/plain":["array([ 1,  2,  5,  7,  9, 10, 11, 13, 14])"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","\n","loss = model(batch).loss.numpy()\n","indices = np.flatnonzero(np.isnan(loss))\n","indices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z2y3Q5N2ZOZj","outputId":"261b82e0-5b3d-401f-d0bf-006d40d4b8dd"},"outputs":[{"data":{"text/plain":["array([[  101,  2007,  2032,  2001,  1037, 16480,  3917,  2594,  4135,\n","        23212,  3070,  2214, 10170,  1010,  2012,  4356,  1997,  3183,\n","         6838, 12953,  2039,  2000,  1996,  6147,  1997,  2010,  2606,\n","         1012,   102,  6838,  2001,  3294,  6625,  3773,  1996,  2214,\n","         2158,  1012,   102,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0],\n","       [  101,  1998,  6814,  2016,  2234,  2461,  2153,  1998, 13322,\n","         2009,  1012,   102,  2045,  1005,  1055,  2053,  3382,  2008,\n","         2016,  1005,  2222,  3046,  8103,  2075,  2009,  2153,  1012,\n","          102,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0],\n","       [  101,  1998,  2007,  1996,  3712,  4634,  1010,  2057,  8108,\n","         2025,  3404,  2028,  1012,  1996,  2616, 18449,  2125,  1999,\n","         1037,  9666,  1997,  4100,  8663, 11020,  6313,  2791,  1998,\n","         2431,  1011,  4301,  1012,   102,  2028,  1005,  1055,  5177,\n","         2110,  1998,  3977,  2000,  2832,  2106,  2025,  2689,  2104,\n","         2122,  6214,  1012,   102,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0],\n","       [  101,  1045,  2001,  1999,  1037, 13090,  5948,  2007,  2048,\n","         2308,  2006,  2026,  5001,  2043,  2026,  2171,  2001,  2170,\n","         1012,   102,  1045,  2001,  3564,  1999,  2277,  1012,   102,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0],\n","       [  101,  2195,  4279,  2191,  2039,  1996,  2181,  2124,  2004,\n","         1996,  2225,  7363,  1012,   102,  2045,  2003,  2069,  2028,\n","         2451,  1999,  1996,  2225,  7363,  1012,   102,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0],\n","       [  101,  2061,  2008,  1045,  2123,  1005,  1056,  2113,  2065,\n","         2009,  2428, 10654,  7347,  2030,  2009,  7126,  2256,  2495,\n","         2291,   102,  2009,  2003,  5094,  2256,  2495,  2291,  2035,\n","         2105,  1012,   102,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0],\n","       [  101,  2051,  1010,  2029,  3216,  2019,  2503,  3444,  1010,\n","         6732,  1996,  2265,  2038, 19840,  2098,  2125,  9906,  1998,\n","         2003,  2770,  2041,  1997,  4784,  1012,   102,  2051,  6732,\n","         1996,  2265,  2003,  9525,  1998,  4569,  1012,   102,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0],\n","       [  101,  1996, 10556,  2140, 11515,  2058,  1010,  2010,  2162,\n","         2252,  5689,  2013,  2010,  7223,  1012,   102,  2043,  1996,\n","        10556,  2140, 11515,  2058,  1010,  2010,  2252,  3062,  2000,\n","         1996,  2598,  1012,   102,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0],\n","       [  101, 13543,  1999,  2049,  6143,  2933,  2443,   102,  2025,\n","        13543,  1999,  6143,  2933,  2003,  2443,   102,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0]])"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["input_ids = batch[\"input_ids\"].numpy()\n","input_ids[indices]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5tstOJM-ZOZj","outputId":"29622a3c-879c-44bb-a912-3cf10146f50a"},"outputs":[{"data":{"text/plain":["2"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["model.config.num_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"66ESgr3SZOZk"},"outputs":[],"source":["from tensorflow.keras.optimizers import Adam\n","\n","model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n","model.compile(optimizer=Adam(5e-5))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yyq7wrvSZOZk","outputId":"e50e537b-2e57-4064-a2fc-989c17f35287"},"outputs":[{"data":{"text/plain":["319/24543 [..............................] - ETA: 16:07 - loss: 0.9718"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(train_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XC98t2P4ZOZk"},"outputs":[],"source":["input_ids = batch[\"input_ids\"].numpy()\n","tokenizer.decode(input_ids[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LbyVkAwkZOZk"},"outputs":[],"source":["labels = batch[\"labels\"].numpy()\n","label = labels[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OUE4OfZdZOZk"},"outputs":[],"source":["for batch in train_dataset:\n","    break\n","\n","# Make sure you have run model.compile() and set your optimizer,\n","# and your loss/metrics if you're using them\n","\n","model.fit(batch, epochs=20)"]}]}