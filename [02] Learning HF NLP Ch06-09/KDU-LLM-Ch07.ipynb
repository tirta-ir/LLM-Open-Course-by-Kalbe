{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNr1GwSRjpx0bOrCYE7uOTZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cplXGT1aV5v-"},"source":["# Token classification"]},{"cell_type":"markdown","metadata":{"id":"l6J0b49ZV5wA"},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nLJUDiPxV5wB"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]\n","!apt install git-lfs"]},{"cell_type":"markdown","metadata":{"id":"vrjNJdD8V5wC"},"source":["You will need to setup git, adapt your email and name in the following cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_VYFdxZGV5wC"},"outputs":[],"source":["!git config --global user.email \"you@example.com\"\n","!git config --global user.name \"Your Name\""]},{"cell_type":"markdown","metadata":{"id":"co0BpgxbV5wC"},"source":["You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0IVtkGq-V5wD"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nx4PmDs0V5wD"},"outputs":[],"source":["from datasets import load_dataset\n","\n","raw_datasets = load_dataset(\"conll2003\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"324X9AbQV5wD","outputId":"be18dafc-8e8f-4133-ba90-b3fd76a9a65e"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],\n","        num_rows: 14041\n","    })\n","    validation: Dataset({\n","        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],\n","        num_rows: 3250\n","    })\n","    test: Dataset({\n","        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],\n","        num_rows: 3453\n","    })\n","})"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["raw_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xhvukYsNV5wE","outputId":"5d2813e6-b8b8-48e5-a09c-fcf653cbb593"},"outputs":[{"data":{"text/plain":["['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["raw_datasets[\"train\"][0][\"tokens\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FZA9NQRaV5wE","outputId":"0c2ea971-5b2d-419a-8076-e36452b38d33"},"outputs":[{"data":{"text/plain":["[3, 0, 7, 0, 0, 0, 7, 0, 0]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["raw_datasets[\"train\"][0][\"ner_tags\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Tx9Et6WV5wE","outputId":"96ac3e32-a065-46de-ee85-13e8fc1e2bf6"},"outputs":[{"data":{"text/plain":["Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n","ner_feature"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lCbk9ZwxV5wE","outputId":"31c0c54f-47d8-40fa-d561-c8ff1f0d4389"},"outputs":[{"data":{"text/plain":["['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["label_names = ner_feature.feature.names\n","label_names"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U4DgFROGV5wF","outputId":"7878852f-9117-4f2e-c058-93d4cc690e32"},"outputs":[{"data":{"text/plain":["'EU    rejects German call to boycott British lamb .'\n","'B-ORG O       B-MISC O    O  O       B-MISC  O    O'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["words = raw_datasets[\"train\"][0][\"tokens\"]\n","labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n","line1 = \"\"\n","line2 = \"\"\n","for word, label in zip(words, labels):\n","    full_label = label_names[label]\n","    max_length = max(len(word), len(full_label))\n","    line1 += word + \" \" * (max_length - len(word) + 1)\n","    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n","\n","print(line1)\n","print(line2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oO9pXRisV5wF"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","model_checkpoint = \"bert-base-cased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GnDXeYcwV5wF","outputId":"e51afebd-2da9-4de2-ae93-39dd2e63445a"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.is_fast"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZzagwtZV5wF","outputId":"1310abb2-c105-4200-e75c-d843f89baa22"},"outputs":[{"data":{"text/plain":["['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n","inputs.tokens()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uoUkZwMTV5wF","outputId":"4b3a72cd-f97e-4af0-8be3-42a14e8251aa"},"outputs":[{"data":{"text/plain":["[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["inputs.word_ids()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HEF4c4ViV5wF"},"outputs":[],"source":["def align_labels_with_tokens(labels, word_ids):\n","    new_labels = []\n","    current_word = None\n","    for word_id in word_ids:\n","        if word_id != current_word:\n","            # Start of a new word!\n","            current_word = word_id\n","            label = -100 if word_id is None else labels[word_id]\n","            new_labels.append(label)\n","        elif word_id is None:\n","            # Special token\n","            new_labels.append(-100)\n","        else:\n","            # Same word as previous token\n","            label = labels[word_id]\n","            # If the label is B-XXX we change it to I-XXX\n","            if label % 2 == 1:\n","                label += 1\n","            new_labels.append(label)\n","\n","    return new_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cv1NAlroV5wF","outputId":"fe53c34b-9a18-4ffd-a49a-66fabe36e4d0"},"outputs":[{"data":{"text/plain":["[3, 0, 7, 0, 0, 0, 7, 0, 0]\n","[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n","word_ids = inputs.word_ids()\n","print(labels)\n","print(align_labels_with_tokens(labels, word_ids))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hDjo0FhfV5wF"},"outputs":[],"source":["def tokenize_and_align_labels(examples):\n","    tokenized_inputs = tokenizer(\n","        examples[\"tokens\"], truncation=True, is_split_into_words=True\n","    )\n","    all_labels = examples[\"ner_tags\"]\n","    new_labels = []\n","    for i, labels in enumerate(all_labels):\n","        word_ids = tokenized_inputs.word_ids(i)\n","        new_labels.append(align_labels_with_tokens(labels, word_ids))\n","\n","    tokenized_inputs[\"labels\"] = new_labels\n","    return tokenized_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bGdfIgiNV5wG"},"outputs":[],"source":["tokenized_datasets = raw_datasets.map(\n","    tokenize_and_align_labels,\n","    batched=True,\n","    remove_columns=raw_datasets[\"train\"].column_names,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EoM5i5JVV5wG"},"outputs":[],"source":["from transformers import DataCollatorForTokenClassification\n","\n","data_collator = DataCollatorForTokenClassification(\n","    tokenizer=tokenizer, return_tensors=\"tf\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q_aCJX13V5wG","outputId":"cb27819a-faf4-47b0-a6fd-e9d0e779aeb1"},"outputs":[{"data":{"text/plain":["tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],\n","        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n","batch[\"labels\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uhNdBN6MV5wG","outputId":"29f223cf-78ce-425f-f5ae-738170d3f45e"},"outputs":[{"data":{"text/plain":["[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n","[-100, 1, 2, -100]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["for i in range(2):\n","    print(tokenized_datasets[\"train\"][i][\"labels\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ksmsht8wV5wG"},"outputs":[],"source":["tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n","    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n","    collate_fn=data_collator,\n","    shuffle=True,\n","    batch_size=16,\n",")\n","\n","tf_eval_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n","    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n","    collate_fn=data_collator,\n","    shuffle=False,\n","    batch_size=16,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6pjAh3JYV5wG"},"outputs":[],"source":["id2label = {i: label for i, label in enumerate(label_names)}\n","label2id = {v: k for k, v in id2label.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"11fqpM3uV5wG"},"outputs":[],"source":["from transformers import TFAutoModelForTokenClassification\n","\n","model = TFAutoModelForTokenClassification.from_pretrained(\n","    model_checkpoint,\n","    id2label=id2label,\n","    label2id=label2id,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fQmM268kV5wG","outputId":"e469a5ae-2530-4be0-f042-f738eff18809"},"outputs":[{"data":{"text/plain":["9"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["model.config.num_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UZ3SUnzIV5wG"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AbtbjuB1V5wG"},"outputs":[],"source":["from transformers import create_optimizer\n","import tensorflow as tf\n","\n","# Train in mixed-precision float16\n","# Comment this line out if you're using a GPU that will not benefit from this\n","tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n","\n","# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n","# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n","# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n","num_epochs = 3\n","num_train_steps = len(tf_train_dataset) * num_epochs\n","\n","optimizer, schedule = create_optimizer(\n","    init_lr=2e-5,\n","    num_warmup_steps=0,\n","    num_train_steps=num_train_steps,\n","    weight_decay_rate=0.01,\n",")\n","model.compile(optimizer=optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-PGv-xonV5wH"},"outputs":[],"source":["from transformers.keras_callbacks import PushToHubCallback\n","\n","callback = PushToHubCallback(output_dir=\"bert-finetuned-ner\", tokenizer=tokenizer)\n","\n","model.fit(\n","    tf_train_dataset,\n","    validation_data=tf_eval_dataset,\n","    callbacks=[callback],\n","    epochs=num_epochs,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a34my8nAV5wH"},"outputs":[],"source":["!pip install seqeval"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"13IJTADHV5wH"},"outputs":[],"source":["import evaluate\n","\n","metric = evaluate.load(\"seqeval\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQotFYwPV5wH","outputId":"0f7e5baf-494e-4cbc-b45b-8f56b25e6801"},"outputs":[{"data":{"text/plain":["['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n","labels = [label_names[i] for i in labels]\n","labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A6UsGzaQV5wH","outputId":"bd345455-55ec-44bd-8237-52fab2ce0351"},"outputs":[{"data":{"text/plain":["{'MISC': {'precision': 1.0, 'recall': 0.5, 'f1': 0.67, 'number': 2},\n"," 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n"," 'overall_precision': 1.0,\n"," 'overall_recall': 0.67,\n"," 'overall_f1': 0.8,\n"," 'overall_accuracy': 0.89}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["predictions = labels.copy()\n","predictions[2] = \"O\"\n","metric.compute(predictions=[predictions], references=[labels])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_YRuq-FmV5wH","outputId":"d150c873-3a54-4bd1-db58-726df562ab2e"},"outputs":[{"data":{"text/plain":["{'LOC': {'precision': 0.91, 'recall': 0.92, 'f1': 0.91, 'number': 1668},\n"," 'MISC': {'precision': 0.70, 'recall': 0.79, 'f1': 0.74, 'number': 702},\n"," 'ORG': {'precision': 0.85, 'recall': 0.90, 'f1': 0.88, 'number': 1661},\n"," 'PER': {'precision': 0.95, 'recall': 0.95, 'f1': 0.95, 'number': 1617},\n"," 'overall_precision': 0.87,\n"," 'overall_recall': 0.91,\n"," 'overall_f1': 0.89,\n"," 'overall_accuracy': 0.97}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","\n","all_predictions = []\n","all_labels = []\n","for batch in tf_eval_dataset:\n","    logits = model.predict_on_batch(batch)[\"logits\"]\n","    labels = batch[\"labels\"]\n","    predictions = np.argmax(logits, axis=-1)\n","    for prediction, label in zip(predictions, labels):\n","        for predicted_idx, label_idx in zip(prediction, label):\n","            if label_idx == -100:\n","                continue\n","            all_predictions.append(label_names[predicted_idx])\n","            all_labels.append(label_names[label_idx])\n","metric.compute(predictions=[all_predictions], references=[all_labels])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FO0HvlWyV5wH","outputId":"953f2a6d-be51-4c05-8f25-43fd4c605013"},"outputs":[{"data":{"text/plain":["[{'entity_group': 'PER', 'score': 0.9988506, 'word': 'Sylvain', 'start': 11, 'end': 18},\n"," {'entity_group': 'ORG', 'score': 0.9647625, 'word': 'Hugging Face', 'start': 33, 'end': 45},\n"," {'entity_group': 'LOC', 'score': 0.9986118, 'word': 'Brooklyn', 'start': 49, 'end': 57}]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import pipeline\n","\n","# Replace this with your own checkpoint\n","model_checkpoint = \"huggingface-course/bert-finetuned-ner\"\n","token_classifier = pipeline(\n","    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",")\n","token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"]},{"cell_type":"markdown","source":[],"metadata":{"id":"I6i9LIc9YlI7"}},{"cell_type":"markdown","metadata":{"id":"1FwJ1ePgV67c"},"source":["# Fine-tuning a masked language model"]},{"cell_type":"markdown","metadata":{"id":"oqlYkseXV67f"},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GuOp6XxzV67f"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]\n","!apt install git-lfs"]},{"cell_type":"markdown","metadata":{"id":"9JzN8AczV67g"},"source":["You will need to setup git, adapt your email and name in the following cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ej2wToVwV67g"},"outputs":[],"source":["!git config --global user.email \"you@example.com\"\n","!git config --global user.name \"Your Name\""]},{"cell_type":"markdown","metadata":{"id":"Favr68REV67h"},"source":["You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TWGAJhVhV67h"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pDJOlknyV67h"},"outputs":[],"source":["from transformers import TFAutoModelForMaskedLM\n","\n","model_checkpoint = \"distilbert-base-uncased\"\n","model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0Ptt5lEV67h","outputId":"1ae246e2-8289-4754-aa1d-683983ef0d1d"},"outputs":[{"data":{"text/plain":["Model: \"tf_distil_bert_for_masked_lm\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","distilbert (TFDistilBertMain multiple                  66362880  \n","_________________________________________________________________\n","vocab_transform (Dense)      multiple                  590592    \n","_________________________________________________________________\n","vocab_layer_norm (LayerNorma multiple                  1536      \n","_________________________________________________________________\n","vocab_projector (TFDistilBer multiple                  23866170  \n","=================================================================\n","Total params: 66,985,530\n","Trainable params: 66,985,530\n","Non-trainable params: 0\n","_________________________________________________________________"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDhZHtAVV67i"},"outputs":[],"source":["text = \"This is a great [MASK].\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yv7lnsnNV67i"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HSYQU_WZV67i","outputId":"f1d51ac3-dd17-4521-d5a1-88636cb43457"},"outputs":[{"data":{"text/plain":["'>>> This is a great deal.'\n","'>>> This is a great success.'\n","'>>> This is a great adventure.'\n","'>>> This is a great idea.'\n","'>>> This is a great feat.'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","import tensorflow as tf\n","\n","inputs = tokenizer(text, return_tensors=\"np\")\n","token_logits = model(**inputs).logits\n","# Find the location of [MASK] and extract its logits\n","mask_token_index = np.argwhere(inputs[\"input_ids\"] == tokenizer.mask_token_id)[0, 1]\n","mask_token_logits = token_logits[0, mask_token_index, :]\n","# Pick the [MASK] candidates with the highest logits\n","# We negate the array before argsort to get the largest, not the smallest, logits\n","top_5_tokens = np.argsort(-mask_token_logits)[:5].tolist()\n","\n","for token in top_5_tokens:\n","    print(f\">>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NnvLMdJKV67j","outputId":"0973b020-62f3-4d44-8f38-873a886b9443"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 25000\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 25000\n","    })\n","    unsupervised: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 50000\n","    })\n","})"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset\n","\n","imdb_dataset = load_dataset(\"imdb\")\n","imdb_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RRaAf0kkV67j","outputId":"4d7d5f78-a84a-4869-f769-52fdad4e5f4e"},"outputs":[{"data":{"text/plain":["\n","'>>> Review: This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, \"kodokoo\" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\'t get me wrong; as clichéd and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'\n","'>>> Label: 0'\n","\n","'>>> Review: Okay, the story makes no sense, the characters lack any dimensionally, the best dialogue is ad-libs about the low quality of movie, the cinematography is dismal, and only editing saves a bit of the muddle, but Sam\" Peckinpah directed the film. Somehow, his direction is not enough. For those who appreciate Peckinpah and his great work, this movie is a disappointment. Even a great cast cannot redeem the time the viewer wastes with this minimal effort.<br /><br />The proper response to the movie is the contempt that the director San Peckinpah, James Caan, Robert Duvall, Burt Young, Bo Hopkins, Arthur Hill, and even Gig Young bring to their work. Watch the great Peckinpah films. Skip this mess.'\n","'>>> Label: 0'\n","\n","'>>> Review: I saw this movie at the theaters when I was about 6 or 7 years old. I loved it then, and have recently come to own a VHS version. <br /><br />My 4 and 6 year old children love this movie and have been asking again and again to watch it. <br /><br />I have enjoyed watching it again too. Though I have to admit it is not as good on a little TV.<br /><br />I do not have older children so I do not know what they would think of it. <br /><br />The songs are very cute. My daughter keeps singing them over and over.<br /><br />Hope this helps.'\n","'>>> Label: 1'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["sample = imdb_dataset[\"train\"].shuffle(seed=42).select(range(3))\n","\n","for row in sample:\n","    print(f\"\\n'>>> Review: {row['text']}'\")\n","    print(f\"'>>> Label: {row['label']}'\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2opRTGnpV67j","outputId":"e0e60842-cb0c-46a6-925d-ae45434fd40b"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['attention_mask', 'input_ids', 'word_ids'],\n","        num_rows: 25000\n","    })\n","    test: Dataset({\n","        features: ['attention_mask', 'input_ids', 'word_ids'],\n","        num_rows: 25000\n","    })\n","    unsupervised: Dataset({\n","        features: ['attention_mask', 'input_ids', 'word_ids'],\n","        num_rows: 50000\n","    })\n","})"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["def tokenize_function(examples):\n","    result = tokenizer(examples[\"text\"])\n","    if tokenizer.is_fast:\n","        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n","    return result\n","\n","\n","# Use batched=True to activate fast multithreading!\n","tokenized_datasets = imdb_dataset.map(\n","    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",")\n","tokenized_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RZ_fmsXnV67j","outputId":"6760e04d-7d0d-436c-f4f4-c63327dccc44"},"outputs":[{"data":{"text/plain":["512"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.model_max_length"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1YRKLBDMV67j"},"outputs":[],"source":["chunk_size = 128"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQOzW_PyV67j","outputId":"aeb4eec1-13ea-4d35-f836-60e46452faef"},"outputs":[{"data":{"text/plain":["'>>> Review 0 length: 200'\n","'>>> Review 1 length: 559'\n","'>>> Review 2 length: 192'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["# Slicing produces a list of lists for each feature\n","tokenized_samples = tokenized_datasets[\"train\"][:3]\n","\n","for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n","    print(f\"'>>> Review {idx} length: {len(sample)}'\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cXNDGcZMV67j","outputId":"20e7b5cf-8c8b-432e-9809-38aab404a9f1"},"outputs":[{"data":{"text/plain":["'>>> Concatenated reviews length: 951'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["concatenated_examples = {\n","    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n","}\n","total_length = len(concatenated_examples[\"input_ids\"])\n","print(f\"'>>> Concatenated reviews length: {total_length}'\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oYvkxj-nV67k","outputId":"61cb9037-8803-4981-c332-a2b6eebb31a8"},"outputs":[{"data":{"text/plain":["'>>> Chunk length: 128'\n","'>>> Chunk length: 128'\n","'>>> Chunk length: 128'\n","'>>> Chunk length: 128'\n","'>>> Chunk length: 128'\n","'>>> Chunk length: 128'\n","'>>> Chunk length: 128'\n","'>>> Chunk length: 55'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["chunks = {\n","    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n","    for k, t in concatenated_examples.items()\n","}\n","\n","for chunk in chunks[\"input_ids\"]:\n","    print(f\"'>>> Chunk length: {len(chunk)}'\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"snTqJRf0V67k"},"outputs":[],"source":["def group_texts(examples):\n","    # Concatenate all texts\n","    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n","    # Compute length of concatenated texts\n","    total_length = len(concatenated_examples[list(examples.keys())[0]])\n","    # We drop the last chunk if it's smaller than chunk_size\n","    total_length = (total_length // chunk_size) * chunk_size\n","    # Split by chunks of max_len\n","    result = {\n","        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n","        for k, t in concatenated_examples.items()\n","    }\n","    # Create a new labels column\n","    result[\"labels\"] = result[\"input_ids\"].copy()\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8a8SKjDGV67k","outputId":"54db5186-ded5-4866-ada7-1c8befd5bdaa"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],\n","        num_rows: 61289\n","    })\n","    test: Dataset({\n","        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],\n","        num_rows: 59905\n","    })\n","    unsupervised: Dataset({\n","        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],\n","        num_rows: 122963\n","    })\n","})"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n","lm_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XcUFXN1JV67k","outputId":"3f56a176-6425-4287-8609-8da1e5066872"},"outputs":[{"data":{"text/plain":["\".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless\""]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CVAiO0JGV67k"},"outputs":[],"source":["from transformers import DataCollatorForLanguageModeling\n","\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I1bGbdf4V67k"},"outputs":[],"source":["samples = [lm_datasets[\"train\"][i] for i in range(2)]\n","for sample in samples:\n","    _ = sample.pop(\"word_ids\")\n","\n","for chunk in data_collator(samples)[\"input_ids\"]:\n","    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nihMKDmPV67k"},"outputs":[],"source":["import collections\n","import numpy as np\n","\n","from transformers.data.data_collator import tf_default_data_collator\n","\n","wwm_probability = 0.2\n","\n","\n","def whole_word_masking_data_collator(features):\n","    for feature in features:\n","        word_ids = feature.pop(\"word_ids\")\n","\n","        # Create a map between words and corresponding token indices\n","        mapping = collections.defaultdict(list)\n","        current_word_index = -1\n","        current_word = None\n","        for idx, word_id in enumerate(word_ids):\n","            if word_id is not None:\n","                if word_id != current_word:\n","                    current_word = word_id\n","                    current_word_index += 1\n","                mapping[current_word_index].append(idx)\n","\n","        # Randomly mask words\n","        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n","        input_ids = feature[\"input_ids\"]\n","        labels = feature[\"labels\"]\n","        new_labels = [-100] * len(labels)\n","        for word_id in np.where(mask)[0]:\n","            word_id = word_id.item()\n","            for idx in mapping[word_id]:\n","                new_labels[idx] = labels[idx]\n","                input_ids[idx] = tokenizer.mask_token_id\n","        feature[\"labels\"] = new_labels\n","\n","    return tf_default_data_collator(features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3E2z1OjNV67k","outputId":"b633f588-ee87-4067-caf4-b9ecfc69ce41"},"outputs":[{"data":{"text/plain":["'>>> [CLS] bromwell high is a cartoon comedy [MASK] it ran at the same time as some other programs about school life, such as \" teachers \". my 35 years in the teaching profession lead me to believe that bromwell high\\'s satire is much closer to reality than is \" teachers \". the scramble to survive financially, the insightful students who can see right through their pathetic teachers\\'pomp, the pettiness of the whole situation, all remind me of the schools i knew and their students. when i saw the episode in which a student repeatedly tried to burn down the school, i immediately recalled.....'\n","\n","'>>> .... [MASK] [MASK] [MASK] [MASK]....... high. a classic line : inspector : i\\'m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn\\'t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["samples = [lm_datasets[\"train\"][i] for i in range(2)]\n","batch = whole_word_masking_data_collator(samples)\n","\n","for chunk in batch[\"input_ids\"]:\n","    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OmynKeDxV67k","outputId":"962c20b9-0c93-4421-ab16-61cbc4934783"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],\n","        num_rows: 10000\n","    })\n","    test: Dataset({\n","        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],\n","        num_rows: 1000\n","    })\n","})"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["train_size = 10_000\n","test_size = int(0.1 * train_size)\n","\n","downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n","    train_size=train_size, test_size=test_size, seed=42\n",")\n","downsampled_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5RrvjiOhV67l"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iv4cTwOzV67l"},"outputs":[],"source":["tf_train_dataset = model.prepare_tf_dataset(\n","    downsampled_dataset[\"train\"],\n","    collate_fn=data_collator,\n","    shuffle=True,\n","    batch_size=32,\n",")\n","\n","tf_eval_dataset = model.prepare_tf_dataset(\n","    downsampled_dataset[\"test\"],\n","    collate_fn=data_collator,\n","    shuffle=False,\n","    batch_size=32,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JjgH75G5V67l"},"outputs":[],"source":["from transformers import create_optimizer\n","from transformers.keras_callbacks import PushToHubCallback\n","import tensorflow as tf\n","\n","num_train_steps = len(tf_train_dataset)\n","optimizer, schedule = create_optimizer(\n","    init_lr=2e-5,\n","    num_warmup_steps=1_000,\n","    num_train_steps=num_train_steps,\n","    weight_decay_rate=0.01,\n",")\n","model.compile(optimizer=optimizer)\n","\n","# Train in mixed-precision float16\n","tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n","\n","model_name = model_checkpoint.split(\"/\")[-1]\n","callback = PushToHubCallback(\n","    output_dir=f\"{model_name}-finetuned-imdb\", tokenizer=tokenizer\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fOxnGo7AV67l","outputId":"9522c72b-ce5d-46f4-a6f8-a8f3f4dc2862"},"outputs":[{"data":{"text/plain":[">>> Perplexity: 21.75"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["import math\n","\n","eval_loss = model.evaluate(tf_eval_dataset)\n","print(f\"Perplexity: {math.exp(eval_loss):.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OTP4oT0HV67l"},"outputs":[],"source":["model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eSrsOJfgV67l","outputId":"ff979402-55cd-4513-fbea-ea0d3fd2f6c6"},"outputs":[{"data":{"text/plain":[">>> Perplexity: 11.32"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["eval_loss = model.evaluate(tf_eval_dataset)\n","print(f\"Perplexity: {math.exp(eval_loss):.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G-Ek6BkIV67l"},"outputs":[],"source":["from transformers import pipeline\n","\n","mask_filler = pipeline(\n","    \"fill-mask\", model=\"huggingface-course/distilbert-base-uncased-finetuned-imdb\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5uQEoNBHV67l","outputId":"a517be16-3690-419e-ffa6-b01538cf9edb"},"outputs":[{"data":{"text/plain":["'>>> this is a great movie.'\n","'>>> this is a great film.'\n","'>>> this is a great story.'\n","'>>> this is a great movies.'\n","'>>> this is a great character.'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["preds = mask_filler(text)\n","\n","for pred in preds:\n","    print(f\">>> {pred['sequence']}\")"]},{"cell_type":"markdown","source":[],"metadata":{"id":"ewhYapT0YvMK"}},{"cell_type":"markdown","metadata":{"id":"rIL5DvBrV7zp"},"source":["# Translation"]},{"cell_type":"markdown","metadata":{"id":"2rNl-jC7V7zs"},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qXXZp5JNV7zt"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]\n","!apt install git-lfs"]},{"cell_type":"markdown","metadata":{"id":"_nIhS7c0V7zu"},"source":["You will need to setup git, adapt your email and name in the following cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sAU3ZvSgV7zu"},"outputs":[],"source":["!git config --global user.email \"you@example.com\"\n","!git config --global user.name \"Your Name\""]},{"cell_type":"markdown","metadata":{"id":"KZxZuEh7V7zu"},"source":["You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_fpU4cTrV7zu"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fdatsRNFV7zv"},"outputs":[],"source":["from datasets import load_dataset\n","\n","raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oguuQSoJV7zv","outputId":"7e2c7d36-bc69-4825-90b5-81e645727507"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'translation'],\n","        num_rows: 210173\n","    })\n","})"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["raw_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l570L9v0V7zv","outputId":"70fa83b0-1803-48ec-d9dc-b463560fb338"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'translation'],\n","        num_rows: 189155\n","    })\n","    test: Dataset({\n","        features: ['id', 'translation'],\n","        num_rows: 21018\n","    })\n","})"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\n","split_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E6nHWS1DV7zw"},"outputs":[],"source":["split_datasets[\"validation\"] = split_datasets.pop(\"test\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eVRRAuYcV7zw","outputId":"9a1d3f50-6947-4b9e-c5ac-71b02f7b0a67"},"outputs":[{"data":{"text/plain":["{'en': 'Default to expanded threads',\n"," 'fr': 'Par défaut, développer les fils de discussion'}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["split_datasets[\"train\"][1][\"translation\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ohKsEvkhV7zw","outputId":"e81df97c-f938-476f-e1c1-89b71b4707ae"},"outputs":[{"data":{"text/plain":["[{'translation_text': 'Par défaut pour les threads élargis'}]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import pipeline\n","\n","model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n","translator = pipeline(\"translation\", model=model_checkpoint)\n","translator(\"Default to expanded threads\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zdrqBkLLV7zw","outputId":"61430d5c-a5e4-4ab7-f857-725e4f23d8f4"},"outputs":[{"data":{"text/plain":["{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',\n"," 'fr': \"Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct.\"}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["split_datasets[\"train\"][172][\"translation\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T5dK4pmwV7zw","outputId":"14187679-1b66-4e23-90f8-e36f6140cc7d"},"outputs":[{"data":{"text/plain":["[{'translation_text': \"Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format.\"}]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["translator(\n","    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zX3nB5mRV7zw"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HdzeX_IdV7zx","outputId":"a8ba040b-f281-466e-f77c-d9bf4563aaf4"},"outputs":[{"data":{"text/plain":["{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["en_sentence = split_datasets[\"train\"][1][\"translation\"][\"en\"]\n","fr_sentence = split_datasets[\"train\"][1][\"translation\"][\"fr\"]\n","\n","inputs = tokenizer(en_sentence, text_target=fr_sentence)\n","inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7VK0715wV7zx","outputId":"64d6f171-5ae7-4286-c512-95839f2a9540"},"outputs":[{"data":{"text/plain":["['▁Par', '▁dé', 'f', 'aut', ',', '▁dé', 've', 'lop', 'per', '▁les', '▁fil', 's', '▁de', '▁discussion', '</s>']\n","['▁Par', '▁défaut', ',', '▁développer', '▁les', '▁fils', '▁de', '▁discussion', '</s>']"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["wrong_targets = tokenizer(fr_sentence)\n","print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n","print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1iRYzkTNV7zx"},"outputs":[],"source":["max_length = 128\n","\n","\n","def preprocess_function(examples):\n","    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n","    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n","    model_inputs = tokenizer(\n","        inputs, text_target=targets, max_length=max_length, truncation=True\n","    )\n","    return model_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MHk-1c_QV7zx"},"outputs":[],"source":["tokenized_datasets = split_datasets.map(\n","    preprocess_function,\n","    batched=True,\n","    remove_columns=split_datasets[\"train\"].column_names,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gyJvacA6V7zx"},"outputs":[],"source":["from transformers import TFAutoModelForSeq2SeqLM\n","\n","model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f4VIL8DeV7zx"},"outputs":[],"source":["from transformers import DataCollatorForSeq2Seq\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQ_C_50dV7zx","outputId":"84464941-c775-49b5-b35d-4602603c2182"},"outputs":[{"data":{"text/plain":["dict_keys(['attention_mask', 'input_ids', 'labels', 'decoder_input_ids'])"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n","batch.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tup7t8TfV7zx","outputId":"986035aa-8c84-43ba-c322-adf7ff172f01"},"outputs":[{"data":{"text/plain":["tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,\n","          -100,  -100,  -100,  -100,  -100,  -100],\n","        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,\n","           550,  7032,  5821,  7907, 12649,     0]])"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["batch[\"labels\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pcftkgMnV7zy","outputId":"ba3ab423-37ac-4b94-f0de-4775f092ceb0"},"outputs":[{"data":{"text/plain":["tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,\n","         59513, 59513, 59513, 59513, 59513, 59513],\n","        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,\n","           817,   550,  7032,  5821,  7907, 12649]])"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["batch[\"decoder_input_ids\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ImVascLTV7zy","outputId":"982efb42-7423-4e25-e347-3537e00ede06"},"outputs":[{"data":{"text/plain":["[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]\n","[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["for i in range(1, 3):\n","    print(tokenized_datasets[\"train\"][i][\"labels\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Uu8dAZvV7zy"},"outputs":[],"source":["tf_train_dataset = model.prepare_tf_dataset(\n","    tokenized_datasets[\"train\"],\n","    collate_fn=data_collator,\n","    shuffle=True,\n","    batch_size=32,\n",")\n","tf_eval_dataset = model.prepare_tf_dataset(\n","    tokenized_datasets[\"validation\"],\n","    collate_fn=data_collator,\n","    shuffle=False,\n","    batch_size=16,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VuoRHxIwV7zy"},"outputs":[],"source":["!pip install sacrebleu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3q2cUPbGV7zy"},"outputs":[],"source":["import evaluate\n","\n","metric = evaluate.load(\"sacrebleu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQxfNn5kV7zy","outputId":"2f9e34ef-440c-47ba-c99d-43c7495a1060"},"outputs":[{"data":{"text/plain":["{'score': 46.750469682990165,\n"," 'counts': [11, 6, 4, 3],\n"," 'totals': [12, 11, 10, 9],\n"," 'precisions': [91.67, 54.54, 40.0, 33.33],\n"," 'bp': 0.9200444146293233,\n"," 'sys_len': 12,\n"," 'ref_len': 13}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["predictions = [\n","    \"This plugin lets you translate web pages between several languages automatically.\"\n","]\n","references = [\n","    [\n","        \"This plugin allows you to automatically translate web pages between several languages.\"\n","    ]\n","]\n","metric.compute(predictions=predictions, references=references)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7973Bh60V7zy","outputId":"1ae6491e-b86c-4f1f-f4ab-aef30954befd"},"outputs":[{"data":{"text/plain":["{'score': 1.683602693167689,\n"," 'counts': [1, 0, 0, 0],\n"," 'totals': [4, 3, 2, 1],\n"," 'precisions': [25.0, 16.67, 12.5, 12.5],\n"," 'bp': 0.10539922456186433,\n"," 'sys_len': 4,\n"," 'ref_len': 13}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["predictions = [\"This This This This\"]\n","references = [\n","    [\n","        \"This plugin allows you to automatically translate web pages between several languages.\"\n","    ]\n","]\n","metric.compute(predictions=predictions, references=references)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AEr2xUBWV7zy","outputId":"f84dc688-dcd3-48fc-fac4-2b7e67ea9b6f"},"outputs":[{"data":{"text/plain":["{'score': 0.0,\n"," 'counts': [2, 1, 0, 0],\n"," 'totals': [2, 1, 0, 0],\n"," 'precisions': [100.0, 100.0, 0.0, 0.0],\n"," 'bp': 0.004086771438464067,\n"," 'sys_len': 2,\n"," 'ref_len': 13}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["predictions = [\"This plugin\"]\n","references = [\n","    [\n","        \"This plugin allows you to automatically translate web pages between several languages.\"\n","    ]\n","]\n","metric.compute(predictions=predictions, references=references)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wln2DUijV7zy"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tqdm import tqdm\n","\n","generation_data_collator = DataCollatorForSeq2Seq(\n","    tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=128\n",")\n","\n","tf_generate_dataset = model.prepare_tf_dataset(\n","    tokenized_datasets[\"validation\"],\n","    collate_fn=generation_data_collator,\n","    shuffle=False,\n","    batch_size=8,\n",")\n","\n","\n","@tf.function(jit_compile=True)\n","def generate_with_xla(batch):\n","    return model.generate(\n","        input_ids=batch[\"input_ids\"],\n","        attention_mask=batch[\"attention_mask\"],\n","        max_new_tokens=128,\n","    )\n","\n","\n","def compute_metrics():\n","    all_preds = []\n","    all_labels = []\n","\n","    for batch, labels in tqdm(tf_generate_dataset):\n","        predictions = generate_with_xla(batch)\n","        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","        labels = labels.numpy()\n","        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","        decoded_preds = [pred.strip() for pred in decoded_preds]\n","        decoded_labels = [[label.strip()] for label in decoded_labels]\n","        all_preds.extend(decoded_preds)\n","        all_labels.extend(decoded_labels)\n","\n","    result = metric.compute(predictions=all_preds, references=all_labels)\n","    return {\"bleu\": result[\"score\"]}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y4LWOC0lV7zz"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pgt5e_Y3V7zz"},"outputs":[],"source":["print(compute_metrics())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K_R9PFGhV7zz"},"outputs":[],"source":["from transformers import create_optimizer\n","from transformers.keras_callbacks import PushToHubCallback\n","import tensorflow as tf\n","\n","# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n","# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n","# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n","num_epochs = 3\n","num_train_steps = len(tf_train_dataset) * num_epochs\n","\n","optimizer, schedule = create_optimizer(\n","    init_lr=5e-5,\n","    num_warmup_steps=0,\n","    num_train_steps=num_train_steps,\n","    weight_decay_rate=0.01,\n",")\n","model.compile(optimizer=optimizer)\n","\n","# Train in mixed-precision float16\n","tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y4PacYKQV7zz"},"outputs":[],"source":["from transformers.keras_callbacks import PushToHubCallback\n","\n","callback = PushToHubCallback(\n","    output_dir=\"marian-finetuned-kde4-en-to-fr\", tokenizer=tokenizer\n",")\n","\n","model.fit(\n","    tf_train_dataset,\n","    validation_data=tf_eval_dataset,\n","    callbacks=[callback],\n","    epochs=num_epochs,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H_F68T19V7zz"},"outputs":[],"source":["print(compute_metrics())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xwUkrX4FV7zz","outputId":"d16e7dbc-1b86-4115-c2af-1ca65cf6693c"},"outputs":[{"data":{"text/plain":["[{'translation_text': 'Par défaut, développer les fils de discussion'}]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import pipeline\n","\n","# Replace this with your own checkpoint\n","model_checkpoint = \"huggingface-course/marian-finetuned-kde4-en-to-fr\"\n","translator = pipeline(\"translation\", model=model_checkpoint)\n","translator(\"Default to expanded threads\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SecJ2RcdV7z6","outputId":"bd4d3b06-ab62-4e57-e9ae-b3c3b6aea664"},"outputs":[{"data":{"text/plain":["[{'translation_text': \"Impossible d'importer %1 en utilisant le module externe d'importation OFX. Ce fichier n'est pas le bon format.\"}]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["translator(\n","    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n",")"]},{"cell_type":"markdown","source":[],"metadata":{"id":"ZUl0mGWbYwsq"}},{"cell_type":"markdown","metadata":{"id":"0thLtXIRV8gn"},"source":["# Summarization\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"flJ5zWOSV8gq"},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ScXYRV5vV8gq"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]\n","!apt install git-lfs"]},{"cell_type":"markdown","metadata":{"id":"d3GmfsrAV8gr"},"source":["You will need to setup git, adapt your email and name in the following cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_fv4UwFNV8gr"},"outputs":[],"source":["!git config --global user.email \"you@example.com\"\n","!git config --global user.name \"Your Name\""]},{"cell_type":"markdown","metadata":{"id":"-N9BxG1OV8gr"},"source":["You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WheBJoQYV8gs"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mvHMWw31V8gs","outputId":"dc40053c-ddc8-415b-b158-6627e1325050"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n","        num_rows: 200000\n","    })\n","    validation: Dataset({\n","        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n","        num_rows: 5000\n","    })\n","    test: Dataset({\n","        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n","        num_rows: 5000\n","    })\n","})"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset\n","\n","spanish_dataset = load_dataset(\"amazon_reviews_multi\", \"es\")\n","english_dataset = load_dataset(\"amazon_reviews_multi\", \"en\")\n","english_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zurNvKuV8gt","outputId":"16ce217f-2797-41f7-c9a6-096efb037636"},"outputs":[{"data":{"text/plain":["'>> Title: Worked in front position, not rear'\n","'>> Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.'\n","\n","'>> Title: meh'\n","'>> Review: Does it’s job and it’s gorgeous but mine is falling apart, I had to basically put it together again with hot glue'\n","\n","'>> Title: Can\\'t beat these for the money'\n","'>> Review: Bought this for handling miscellaneous aircraft parts and hanger \"stuff\" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn\\'t get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\\'s heavy duty enough to hold metal parts, but being made of plastic it\\'s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\\'t beat it. Best one of these I\\'ve bought to date-- and I\\'ve been using some version of these for over forty years.'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["def show_samples(dataset, num_samples=3, seed=42):\n","    sample = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))\n","    for example in sample:\n","        print(f\"\\n'>> Title: {example['review_title']}'\")\n","        print(f\"'>> Review: {example['review_body']}'\")\n","\n","\n","show_samples(english_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xh6MZnt5V8gt","outputId":"741b6600-2df7-4451-d178-3577b81609d8"},"outputs":[{"data":{"text/plain":["home                      17679\n","apparel                   15951\n","wireless                  15717\n","other                     13418\n","beauty                    12091\n","drugstore                 11730\n","kitchen                   10382\n","toy                        8745\n","sports                     8277\n","automotive                 7506\n","lawn_and_garden            7327\n","home_improvement           7136\n","pet_products               7082\n","digital_ebook_purchase     6749\n","pc                         6401\n","electronics                6186\n","office_product             5521\n","shoes                      5197\n","grocery                    4730\n","book                       3756\n","Name: product_category, dtype: int64"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["english_dataset.set_format(\"pandas\")\n","english_df = english_dataset[\"train\"][:]\n","# Show counts for top 20 products\n","english_df[\"product_category\"].value_counts()[:20]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SIVGsq-_V8gt"},"outputs":[],"source":["def filter_books(example):\n","    return (\n","        example[\"product_category\"] == \"book\"\n","        or example[\"product_category\"] == \"digital_ebook_purchase\"\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5nE8a6npV8gt"},"outputs":[],"source":["english_dataset.reset_format()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ngY2Mv6vV8gu","outputId":"5416eed3-b9af-4ee0-ef84-59604b7abd7c"},"outputs":[{"data":{"text/plain":["'>> Title: I\\'m dissapointed.'\n","'>> Review: I guess I had higher expectations for this book from the reviews. I really thought I\\'d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\\'m dissapointed.'\n","\n","'>> Title: Good art, good price, poor design'\n","'>> Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\\'s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar'\n","\n","'>> Title: Helpful'\n","'>> Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["spanish_books = spanish_dataset.filter(filter_books)\n","english_books = english_dataset.filter(filter_books)\n","show_samples(english_books)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MLUitA_9V8gu","outputId":"b621913c-a5b6-45b9-c537-cca67634579c"},"outputs":[{"data":{"text/plain":["'>> Title: Easy to follow!!!!'\n","'>> Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.'\n","\n","'>> Title: PARCIALMENTE DAÑADO'\n","'>> Review: Me llegó el día que tocaba, junto a otros libros que pedí, pero la caja llegó en mal estado lo cual dañó las esquinas de los libros porque venían sin protección (forro).'\n","\n","'>> Title: no lo he podido descargar'\n","'>> Review: igual que el anterior'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import concatenate_datasets, DatasetDict\n","\n","books_dataset = DatasetDict()\n","\n","for split in english_books.keys():\n","    books_dataset[split] = concatenate_datasets(\n","        [english_books[split], spanish_books[split]]\n","    )\n","    books_dataset[split] = books_dataset[split].shuffle(seed=42)\n","\n","# Peek at a few examples\n","show_samples(books_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"siMkdb2cV8gu"},"outputs":[],"source":["books_dataset = books_dataset.filter(lambda x: len(x[\"review_title\"].split()) > 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P3ILgGlTV8gu"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","model_checkpoint = \"google/mt5-small\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kc0gurKNV8gu","outputId":"da8a1776-6439-439b-fbde-48e2ced8c8b7"},"outputs":[{"data":{"text/plain":["{'input_ids': [336, 259, 28387, 11807, 287, 62893, 295, 12507, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["inputs = tokenizer(\"I loved reading the Hunger Games!\")\n","inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VN3ahJ1rV8gu","outputId":"7812649e-1c25-4cb0-e95a-5e484acfe374"},"outputs":[{"data":{"text/plain":["['▁I', '▁', 'loved', '▁reading', '▁the', '▁Hung', 'er', '▁Games', '</s>']"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.convert_ids_to_tokens(inputs.input_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JYRJ6j_CV8gv"},"outputs":[],"source":["max_input_length = 512\n","max_target_length = 30\n","\n","\n","def preprocess_function(examples):\n","    model_inputs = tokenizer(\n","        examples[\"review_body\"],\n","        max_length=max_input_length,\n","        truncation=True,\n","    )\n","    labels = tokenizer(\n","        examples[\"review_title\"], max_length=max_target_length, truncation=True\n","    )\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9OBbPOUyV8gv"},"outputs":[],"source":["tokenized_datasets = books_dataset.map(preprocess_function, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VLGEauLRV8gv"},"outputs":[],"source":["generated_summary = \"I absolutely loved reading the Hunger Games\"\n","reference_summary = \"I loved reading the Hunger Games\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bq2vwFy1V8gv"},"outputs":[],"source":["!pip install rouge_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l6Ydsex5V8gv"},"outputs":[],"source":["import evaluate\n","\n","rouge_score = evaluate.load(\"rouge\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7qhxyqO_V8gv","outputId":"5aa3e5b1-975f-45f6-a794-7d9b37aab049"},"outputs":[{"data":{"text/plain":["{'rouge1': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),\n"," 'rouge2': AggregateScore(low=Score(precision=0.67, recall=0.8, fmeasure=0.73), mid=Score(precision=0.67, recall=0.8, fmeasure=0.73), high=Score(precision=0.67, recall=0.8, fmeasure=0.73)),\n"," 'rougeL': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),\n"," 'rougeLsum': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92))}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["scores = rouge_score.compute(\n","    predictions=[generated_summary], references=[reference_summary]\n",")\n","scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NBiVKBZbV8gv","outputId":"c90386d5-4430-4804-c8fc-1e14d8c85641"},"outputs":[{"data":{"text/plain":["Score(precision=0.86, recall=1.0, fmeasure=0.92)"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["scores[\"rouge1\"].mid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qcqAo32xV8gv"},"outputs":[],"source":["!pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4JSXqkICV8gv"},"outputs":[],"source":["import nltk\n","\n","nltk.download(\"punkt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Epa-RwZV8gw","outputId":"42ad6083-c957-4075-988c-4ea98301517e"},"outputs":[{"data":{"text/plain":["'I grew up reading Koontz, and years ago, I stopped,convinced i had \"outgrown\" him.'\n","'Still,when a friend was looking for something suspenseful too read, I suggested Koontz.'\n","'She found Strangers.'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.tokenize import sent_tokenize\n","\n","\n","def three_sentence_summary(text):\n","    return \"\\n\".join(sent_tokenize(text)[:3])\n","\n","\n","print(three_sentence_summary(books_dataset[\"train\"][1][\"review_body\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LioJXFs9V8gw"},"outputs":[],"source":["def evaluate_baseline(dataset, metric):\n","    summaries = [three_sentence_summary(text) for text in dataset[\"review_body\"]]\n","    return metric.compute(predictions=summaries, references=dataset[\"review_title\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RMAi1g2TV8gw","outputId":"14eb158b-e4cd-416a-e5f5-5ed3661cb354"},"outputs":[{"data":{"text/plain":["{'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","score = evaluate_baseline(books_dataset[\"validation\"], rouge_score)\n","rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n","rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names)\n","rouge_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T085MYcqV8gw"},"outputs":[],"source":["from transformers import TFAutoModelForSeq2SeqLM\n","\n","model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4y1Weo8ZV8gw"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMNco7opV8gw"},"outputs":[],"source":["from transformers import DataCollatorForSeq2Seq\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4xjq5HgSV8gw"},"outputs":[],"source":["tokenized_datasets = tokenized_datasets.remove_columns(\n","    books_dataset[\"train\"].column_names\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t5n8kttCV8gw","outputId":"52244c72-2ca7-42a3-8b1b-367c6e0bd082"},"outputs":[{"data":{"text/plain":["{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[  1494,    259,   8622,    390,    259,    262,   2316,   3435,    955,\n","            772,    281,    772,   1617,    263,    305,  14701,    260,   1385,\n","           3031,    259,  24146,    332,   1037,    259,  43906,    305,    336,\n","            260,      1,      0,      0,      0,      0,      0,      0],\n","        [   259,  27531,  13483,    259,   7505,    260, 112240,  15192,    305,\n","          53198,    276,    259,  74060,    263,    260,    459,  25640,    776,\n","           2119,    336,    259,   2220,    259,  18896,    288,   4906,    288,\n","           1037,   3931,    260,   7083, 101476,   1143,    260,      1]]), 'labels': tensor([[ 7483,   259,  2364, 15695,     1,  -100],\n","        [  259, 27531, 13483,   259,  7505,     1]]), 'decoder_input_ids': tensor([[    0,  7483,   259,  2364, 15695,     1],\n","        [    0,   259, 27531, 13483,   259,  7505]])}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["features = [tokenized_datasets[\"train\"][i] for i in range(2)]\n","data_collator(features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"urJYnMNEV8gw"},"outputs":[],"source":["tf_train_dataset = model.prepare_tf_dataset(\n","    tokenized_datasets[\"train\"],\n","    collate_fn=data_collator,\n","    shuffle=True,\n","    batch_size=8,\n",")\n","tf_eval_dataset = model.prepare_tf_dataset(\n","    tokenized_datasets[\"validation\"],\n","    collate_fn=data_collator,\n","    shuffle=False,\n","    batch_size=8,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3hViw3OV8gx"},"outputs":[],"source":["from transformers import create_optimizer\n","import tensorflow as tf\n","\n","# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n","# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n","# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n","num_train_epochs = 8\n","num_train_steps = len(tf_train_dataset) * num_train_epochs\n","model_name = model_checkpoint.split(\"/\")[-1]\n","\n","optimizer, schedule = create_optimizer(\n","    init_lr=5.6e-5,\n","    num_warmup_steps=0,\n","    num_train_steps=num_train_steps,\n","    weight_decay_rate=0.01,\n",")\n","\n","model.compile(optimizer=optimizer)\n","\n","# Train in mixed-precision float16\n","tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VVMSNArUV8gx"},"outputs":[],"source":["from transformers.keras_callbacks import PushToHubCallback\n","\n","callback = PushToHubCallback(\n","    output_dir=f\"{model_name}-finetuned-amazon-en-es\", tokenizer=tokenizer\n",")\n","\n","model.fit(\n","    tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback], epochs=8\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8GKObcyV8gx"},"outputs":[],"source":["from tqdm import tqdm\n","import numpy as np\n","\n","generation_data_collator = DataCollatorForSeq2Seq(\n","    tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=320\n",")\n","\n","tf_generate_dataset = model.prepare_tf_dataset(\n","    tokenized_datasets[\"validation\"],\n","    collate_fn=generation_data_collator,\n","    shuffle=False,\n","    batch_size=8,\n","    drop_remainder=True,\n",")\n","\n","\n","@tf.function(jit_compile=True)\n","def generate_with_xla(batch):\n","    return model.generate(\n","        input_ids=batch[\"input_ids\"],\n","        attention_mask=batch[\"attention_mask\"],\n","        max_new_tokens=32,\n","    )\n","\n","\n","all_preds = []\n","all_labels = []\n","for batch, labels in tqdm(tf_generate_dataset):\n","    predictions = generate_with_xla(batch)\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    labels = labels.numpy()\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n","    all_preds.extend(decoded_preds)\n","    all_labels.extend(decoded_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kvfw0Bu9V8g4"},"outputs":[],"source":["result = rouge_score.compute(\n","    predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",")\n","result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n","{k: round(v, 4) for k, v in result.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QPdPm9XAV8g4"},"outputs":[],"source":["from transformers import pipeline\n","\n","hub_model_id = \"huggingface-course/mt5-small-finetuned-amazon-en-es\"\n","summarizer = pipeline(\"summarization\", model=hub_model_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gT-sTeUUV8g4"},"outputs":[],"source":["def print_summary(idx):\n","    review = books_dataset[\"test\"][idx][\"review_body\"]\n","    title = books_dataset[\"test\"][idx][\"review_title\"]\n","    summary = summarizer(books_dataset[\"test\"][idx][\"review_body\"])[0][\"summary_text\"]\n","    print(f\"'>>> Review: {review}'\")\n","    print(f\"\\n'>>> Title: {title}'\")\n","    print(f\"\\n'>>> Summary: {summary}'\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DEErp-wmV8g4","outputId":"395bf894-863c-4fc8-9473-4bd2a69b84ba"},"outputs":[{"data":{"text/plain":["'>>> Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesn’t come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. It’s also really expensive for what it is.'\n","\n","'>>> Title: Not impressed at all... buy something else'\n","\n","'>>> Summary: Nothing special at all about this product'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["print_summary(100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W1G3S9W_V8g4","outputId":"5e54c1dc-cd67-429f-dc3c-1457fd962077"},"outputs":[{"data":{"text/plain":["'>>> Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada'\n","\n","'>>> Title: Buena literatura para adolescentes'\n","\n","'>>> Summary: Muy facil de leer'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["print_summary(0)"]},{"cell_type":"markdown","source":[],"metadata":{"id":"z9lnhf-HYymi"}},{"cell_type":"markdown","metadata":{"id":"QUh0dnV7V9m-"},"source":["# Training a causal language model from scratch"]},{"cell_type":"markdown","metadata":{"id":"au5qKyAvV9nB"},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcdUqYUCV9nC"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]\n","!apt install git-lfs"]},{"cell_type":"markdown","metadata":{"id":"4TBtPUoaV9nE"},"source":["You will need to setup git, adapt your email and name in the following cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Hxr0_1nV9nE"},"outputs":[],"source":["!git config --global user.email \"you@example.com\"\n","!git config --global user.name \"Your Name\""]},{"cell_type":"markdown","metadata":{"id":"gkcecxbMV9nE"},"source":["You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"btwBz8kmV9nF"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dXZe_GzLV9nF"},"outputs":[],"source":["def any_keyword_in_string(string, keywords):\n","    for keyword in keywords:\n","        if keyword in string:\n","            return True\n","    return False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ODkQETatV9nF","outputId":"a0de8e3a-4f88-477c-dbca-840d80eb2c04"},"outputs":[{"data":{"text/plain":["False True"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n","example_1 = \"import numpy as np\"\n","example_2 = \"import pandas as pd\"\n","\n","print(\n","    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"onyM6bXjV9nG"},"outputs":[],"source":["from collections import defaultdict\n","from tqdm import tqdm\n","from datasets import Dataset\n","\n","\n","def filter_streaming_dataset(dataset, filters):\n","    filtered_dict = defaultdict(list)\n","    total = 0\n","    for sample in tqdm(iter(dataset)):\n","        total += 1\n","        if any_keyword_in_string(sample[\"content\"], filters):\n","            for k, v in sample.items():\n","                filtered_dict[k].append(v)\n","    print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n","    return Dataset.from_dict(filtered_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CUWiPASPV9nH","outputId":"d77ffb4a-e630-4661-854d-315482a6a59e"},"outputs":[{"data":{"text/plain":["3.26% of data after filtering."]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["# This cell will take a very long time to execute, so you should skip it and go to\n","# the next one!\n","from datasets import load_dataset\n","\n","split = \"train\"  # \"valid\"\n","filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n","\n","data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n","filtered_data = filter_streaming_dataset(data, filters)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_elhh2sXV9nH","outputId":"d4e8626d-e578-4bac-a4ac-04e46bce39c3"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n","        num_rows: 606720\n","    })\n","    valid: Dataset({\n","        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n","        num_rows: 3322\n","    })\n","})"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset, DatasetDict\n","\n","ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n","ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n","\n","raw_datasets = DatasetDict(\n","    {\n","        \"train\": ds_train,  # .shuffle().select(range(50000)),\n","        \"valid\": ds_valid,  # .shuffle().select(range(500))\n","    }\n",")\n","\n","raw_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6GZxLD8V9nH","outputId":"546352e2-5af8-47c9-c77b-9b8651b9b22f"},"outputs":[{"data":{"text/plain":["'REPO_NAME: kmike/scikit-learn'\n","'PATH: sklearn/utils/__init__.py'\n","'COPIES: 3'\n","'SIZE: 10094'\n","'''CONTENT: \"\"\"\n","The :mod:`sklearn.utils` module includes various utilites.\n","\"\"\"\n","\n","from collections import Sequence\n","\n","import numpy as np\n","from scipy.sparse import issparse\n","import warnings\n","\n","from .murmurhash import murm\n","LICENSE: bsd-3-clause'''"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["for key in raw_datasets[\"train\"][0]:\n","    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fx2HR_6cV9nH","outputId":"d2b06e5e-0e36-4d4d-f035-57b629858ebd"},"outputs":[{"data":{"text/plain":["Input IDs length: 34\n","Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]\n","Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import AutoTokenizer\n","\n","context_length = 128\n","tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n","\n","outputs = tokenizer(\n","    raw_datasets[\"train\"][:2][\"content\"],\n","    truncation=True,\n","    max_length=context_length,\n","    return_overflowing_tokens=True,\n","    return_length=True,\n",")\n","\n","print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n","print(f\"Input chunk lengths: {(outputs['length'])}\")\n","print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzod4e4hV9nI","outputId":"1972f584-01bf-438c-b747-8f3149d4312e"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['input_ids'],\n","        num_rows: 16702061\n","    })\n","    valid: Dataset({\n","        features: ['input_ids'],\n","        num_rows: 93164\n","    })\n","})"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["def tokenize(element):\n","    outputs = tokenizer(\n","        element[\"content\"],\n","        truncation=True,\n","        max_length=context_length,\n","        return_overflowing_tokens=True,\n","        return_length=True,\n","    )\n","    input_batch = []\n","    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n","        if length == context_length:\n","            input_batch.append(input_ids)\n","    return {\"input_ids\": input_batch}\n","\n","\n","tokenized_datasets = raw_datasets.map(\n","    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",")\n","tokenized_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WaV27izOV9nI"},"outputs":[],"source":["from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig\n","\n","config = AutoConfig.from_pretrained(\n","    \"gpt2\",\n","    vocab_size=len(tokenizer),\n","    n_ctx=context_length,\n","    bos_token_id=tokenizer.bos_token_id,\n","    eos_token_id=tokenizer.eos_token_id,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tV7YJ1UTV9nI","outputId":"76a48308-74ff-433d-87c0-b5e8f45a9b07"},"outputs":[{"data":{"text/plain":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","transformer (TFGPT2MainLayer multiple                  124242432 \n","=================================================================\n","Total params: 124,242,432\n","Trainable params: 124,242,432\n","Non-trainable params: 0\n","_________________________________________________________________"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["model = TFGPT2LMHeadModel(config)\n","model(model.dummy_inputs)  # Builds the model\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LJXkQpTdV9nI"},"outputs":[],"source":["from transformers import DataCollatorForLanguageModeling\n","\n","tokenizer.pad_token = tokenizer.eos_token\n","data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"tf\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ur6IiVvfV9nI","outputId":"a395038d-50ee-4c64-aaad-c26e96a1aecd"},"outputs":[{"data":{"text/plain":["input_ids shape: (5, 128)\n","attention_mask shape: (5, 128)\n","labels shape: (5, 128)"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n","for key in out:\n","    print(f\"{key} shape: {out[key].shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b4i_KjhNV9nI"},"outputs":[],"source":["tf_train_dataset = model.prepare_tf_dataset(\n","    tokenized_dataset[\"train\"],\n","    collate_fn=data_collator,\n","    shuffle=True,\n","    batch_size=32,\n",")\n","tf_eval_dataset = model.prepare_tf_dataset(\n","    tokenized_dataset[\"valid\"],\n","    collate_fn=data_collator,\n","    shuffle=False,\n","    batch_size=32,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xf5Ou7foV9nJ"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6scYPoyfV9nJ"},"outputs":[],"source":["from transformers import create_optimizer\n","import tensorflow as tf\n","\n","num_train_steps = len(tf_train_dataset)\n","optimizer, schedule = create_optimizer(\n","    init_lr=5e-5,\n","    num_warmup_steps=1_000,\n","    num_train_steps=num_train_steps,\n","    weight_decay_rate=0.01,\n",")\n","model.compile(optimizer=optimizer)\n","\n","# Train in mixed-precision float16\n","tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1_rZQzzYV9nJ"},"outputs":[],"source":["from transformers.keras_callbacks import PushToHubCallback\n","\n","callback = PushToHubCallback(output_dir=\"codeparrot-ds\", tokenizer=tokenizer)\n","\n","model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9FpNPb5VV9nJ"},"outputs":[],"source":["from transformers import pipeline\n","\n","course_model = TFGPT2LMHeadModel.from_pretrained(\"huggingface-course/codeparrot-ds\")\n","course_tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/codeparrot-ds\")\n","pipe = pipeline(\n","    \"text-generation\", model=course_model, tokenizer=course_tokenizer, device=0\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CYtn_9qtV9nJ","outputId":"3c28b72e-77fc-4b89-d5e6-78ff250bc3f7"},"outputs":[{"data":{"text/plain":["# create some data\n","x = np.random.randn(100)\n","y = np.random.randn(100)\n","\n","# create scatter plot with x, y\n","plt.scatter(x, y)\n","\n","# create scatter"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["txt = \"\"\"\\\n","# create some data\n","x = np.random.randn(100)\n","y = np.random.randn(100)\n","\n","# create scatter plot with x, y\n","\"\"\"\n","print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HoCS-7-RV9nJ","outputId":"517222d0-81bf-4a5f-a4ed-6edd2caebf6f"},"outputs":[{"data":{"text/plain":["# create some data\n","x = np.random.randn(100)\n","y = np.random.randn(100)\n","\n","# create dataframe from x and y\n","df = pd.DataFrame({'x': x, 'y': y})\n","df.insert(0,'x', x)\n","for"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["txt = \"\"\"\\\n","# create some data\n","x = np.random.randn(100)\n","y = np.random.randn(100)\n","\n","# create dataframe from x and y\n","\"\"\"\n","print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6UmmaJbgV9nJ","outputId":"dc476624-d7f4-4bb4-8027-b00a9b17d903"},"outputs":[{"data":{"text/plain":["# dataframe with profession, income and name\n","df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n","\n","# calculate the mean income per profession\n","profession = df.groupby(['profession']).mean()\n","\n","# compute the"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["txt = \"\"\"\\\n","# dataframe with profession, income and name\n","df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n","\n","# calculate the mean income per profession\n","\"\"\"\n","print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BgSg-QZ4V9nJ","outputId":"c0f76813-ad16-4d10-b780-fe5768510dcb"},"outputs":[{"data":{"text/plain":["# import random forest regressor from scikit-learn\n","from sklearn.ensemble import RandomForestRegressor\n","\n","# fit random forest model with 300 estimators on X, y:\n","rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)\n","rf.fit(X, y)\n","rf"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["txt = \"\"\"\n","# import random forest regressor from scikit-learn\n","from sklearn.ensemble import RandomForestRegressor\n","\n","# fit random forest model with 300 estimators on X, y:\n","\"\"\"\n","print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"]},{"cell_type":"markdown","source":[],"metadata":{"id":"YHo6WugZY0Wa"}},{"cell_type":"markdown","metadata":{"id":"omEqQTHdV-a9"},"source":["# Question answering\n"]},{"cell_type":"markdown","metadata":{"id":"QJop_usAV-bA"},"source":["Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qsD4jx4wV-bB"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]\n","!apt install git-lfs"]},{"cell_type":"markdown","metadata":{"id":"dJCA3hYPV-bC"},"source":["You will need to setup git, adapt your email and name in the following cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pxs7Hjw-V-bC"},"outputs":[],"source":["!git config --global user.email \"you@example.com\"\n","!git config --global user.name \"Your Name\""]},{"cell_type":"markdown","metadata":{"id":"qCAxn1Q3V-bC"},"source":["You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgSyv8GIV-bD"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WxtVgM4tV-bD"},"outputs":[],"source":["from datasets import load_dataset\n","\n","raw_datasets = load_dataset(\"squad\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"schR7yOWV-bE","outputId":"0234fb9f-0c86-428d-a7d7-8f952a6267ef"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'title', 'context', 'question', 'answers'],\n","        num_rows: 87599\n","    })\n","    validation: Dataset({\n","        features: ['id', 'title', 'context', 'question', 'answers'],\n","        num_rows: 10570\n","    })\n","})"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["raw_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R3GpGPLYV-bE","outputId":"2f28e80a-4f41-49d1-ace5-00e5dacf133e"},"outputs":[{"data":{"text/plain":["Context: 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'\n","Question: 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'\n","Answer: {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["print(\"Context: \", raw_datasets[\"train\"][0][\"context\"])\n","print(\"Question: \", raw_datasets[\"train\"][0][\"question\"])\n","print(\"Answer: \", raw_datasets[\"train\"][0][\"answers\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqbLbhK5V-bE","outputId":"2ff1aa23-ecde-43eb-d8fc-aeb1ac7957d2"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['id', 'title', 'context', 'question', 'answers'],\n","    num_rows: 0\n","})"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["raw_datasets[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KabCVq2vV-bF","outputId":"c2cf1d68-b17c-4418-ab58-979b2600a33c"},"outputs":[{"data":{"text/plain":["{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}\n","{'text': ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"], 'answer_start': [403, 355, 355]}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["print(raw_datasets[\"validation\"][0][\"answers\"])\n","print(raw_datasets[\"validation\"][2][\"answers\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"11VOowPXV-bF","outputId":"2af5c838-3411-46c8-dd6e-376da90d5ce7"},"outputs":[{"data":{"text/plain":["'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.'\n","'Where did Super Bowl 50 take place?'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["print(raw_datasets[\"validation\"][2][\"context\"])\n","print(raw_datasets[\"validation\"][2][\"question\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W3zmnnKnV-bF"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","model_checkpoint = \"bert-base-cased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i8JpbyobV-bF","outputId":"0dd3f512-ffdc-453c-93a4-50d861255c67"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.is_fast"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NthnS7prV-bF","outputId":"a0925b47-912f-4494-f470-e9fdc44c6a2c"},"outputs":[{"data":{"text/plain":["'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, '\n","'the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin '\n","'Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms '\n","'upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred '\n","'Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a '\n","'replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette '\n","'Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues '\n","'and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["context = raw_datasets[\"train\"][0][\"context\"]\n","question = raw_datasets[\"train\"][0][\"question\"]\n","\n","inputs = tokenizer(question, context)\n","tokenizer.decode(inputs[\"input_ids\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wzTz0gMpV-bF","outputId":"15a05b97-1374-456a-fcd9-56af0dfc5994"},"outputs":[{"data":{"text/plain":["'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP]'\n","'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP]'\n","'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP]'\n","'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["inputs = tokenizer(\n","    question,\n","    context,\n","    max_length=100,\n","    truncation=\"only_second\",\n","    stride=50,\n","    return_overflowing_tokens=True,\n",")\n","\n","for ids in inputs[\"input_ids\"]:\n","    print(tokenizer.decode(ids))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zT7xsvVmV-bG","outputId":"b9776084-3a2a-4f34-fa18-00a2878d8d77"},"outputs":[{"data":{"text/plain":["dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["inputs = tokenizer(\n","    question,\n","    context,\n","    max_length=100,\n","    truncation=\"only_second\",\n","    stride=50,\n","    return_overflowing_tokens=True,\n","    return_offsets_mapping=True,\n",")\n","inputs.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mfkFpBLV-bG","outputId":"eba67d71-2587-4978-8f21-ed3482a4c3ac"},"outputs":[{"data":{"text/plain":["[0, 0, 0, 0]"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["inputs[\"overflow_to_sample_mapping\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"otVkx3qDV-bG","outputId":"c6394462-7a80-4d09-951e-56de0a93c3af"},"outputs":[{"data":{"text/plain":["'The 4 examples gave 19 features.'\n","'Here is where each comes from: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3].'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["inputs = tokenizer(\n","    raw_datasets[\"train\"][2:6][\"question\"],\n","    raw_datasets[\"train\"][2:6][\"context\"],\n","    max_length=100,\n","    truncation=\"only_second\",\n","    stride=50,\n","    return_overflowing_tokens=True,\n","    return_offsets_mapping=True,\n",")\n","\n","print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\n","print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q8XTcFWyV-bG","outputId":"3d65fac9-578c-4418-d932-d603e7e41903"},"outputs":[{"data":{"text/plain":["([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0],\n"," [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0])"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["answers = raw_datasets[\"train\"][2:6][\"answers\"]\n","start_positions = []\n","end_positions = []\n","\n","for i, offset in enumerate(inputs[\"offset_mapping\"]):\n","    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]\n","    answer = answers[sample_idx]\n","    start_char = answer[\"answer_start\"][0]\n","    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n","    sequence_ids = inputs.sequence_ids(i)\n","\n","    # Find the start and end of the context\n","    idx = 0\n","    while sequence_ids[idx] != 1:\n","        idx += 1\n","    context_start = idx\n","    while sequence_ids[idx] == 1:\n","        idx += 1\n","    context_end = idx - 1\n","\n","    # If the answer is not fully inside the context, label is (0, 0)\n","    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n","        start_positions.append(0)\n","        end_positions.append(0)\n","    else:\n","        # Otherwise it's the start and end token positions\n","        idx = context_start\n","        while idx <= context_end and offset[idx][0] <= start_char:\n","            idx += 1\n","        start_positions.append(idx - 1)\n","\n","        idx = context_end\n","        while idx >= context_start and offset[idx][1] >= end_char:\n","            idx -= 1\n","        end_positions.append(idx + 1)\n","\n","start_positions, end_positions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cwvz0_-ZV-bG","outputId":"0705ecad-15d2-4c38-fe40-c7d244492845"},"outputs":[{"data":{"text/plain":["'Theoretical answer: the Main Building, labels give: the Main Building'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["idx = 0\n","sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n","answer = answers[sample_idx][\"text\"][0]\n","\n","start = start_positions[idx]\n","end = end_positions[idx]\n","labeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n","\n","print(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o1ShIRYjV-bH","outputId":"ff0add06-7943-4153-bc68-ddbbe1df37f2"},"outputs":[{"data":{"text/plain":["'Theoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP]'"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["idx = 4\n","sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n","answer = answers[sample_idx][\"text\"][0]\n","\n","decoded_example = tokenizer.decode(inputs[\"input_ids\"][idx])\n","print(f\"Theoretical answer: {answer}, decoded example: {decoded_example}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bAoN-hbVV-bH"},"outputs":[],"source":["max_length = 384\n","stride = 128\n","\n","\n","def preprocess_training_examples(examples):\n","    questions = [q.strip() for q in examples[\"question\"]]\n","    inputs = tokenizer(\n","        questions,\n","        examples[\"context\"],\n","        max_length=max_length,\n","        truncation=\"only_second\",\n","        stride=stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    offset_mapping = inputs.pop(\"offset_mapping\")\n","    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n","    answers = examples[\"answers\"]\n","    start_positions = []\n","    end_positions = []\n","\n","    for i, offset in enumerate(offset_mapping):\n","        sample_idx = sample_map[i]\n","        answer = answers[sample_idx]\n","        start_char = answer[\"answer_start\"][0]\n","        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n","        sequence_ids = inputs.sequence_ids(i)\n","\n","        # Find the start and end of the context\n","        idx = 0\n","        while sequence_ids[idx] != 1:\n","            idx += 1\n","        context_start = idx\n","        while sequence_ids[idx] == 1:\n","            idx += 1\n","        context_end = idx - 1\n","\n","        # If the answer is not fully inside the context, label is (0, 0)\n","        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n","            start_positions.append(0)\n","            end_positions.append(0)\n","        else:\n","            # Otherwise it's the start and end token positions\n","            idx = context_start\n","            while idx <= context_end and offset[idx][0] <= start_char:\n","                idx += 1\n","            start_positions.append(idx - 1)\n","\n","            idx = context_end\n","            while idx >= context_start and offset[idx][1] >= end_char:\n","                idx -= 1\n","            end_positions.append(idx + 1)\n","\n","    inputs[\"start_positions\"] = start_positions\n","    inputs[\"end_positions\"] = end_positions\n","    return inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DIbsSl32V-bH","outputId":"d35a3f72-9211-487e-c7bb-7f674931a770"},"outputs":[{"data":{"text/plain":["(87599, 88729)"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset = raw_datasets[\"train\"].map(\n","    preprocess_training_examples,\n","    batched=True,\n","    remove_columns=raw_datasets[\"train\"].column_names,\n",")\n","len(raw_datasets[\"train\"]), len(train_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1q3OL_pCV-bH"},"outputs":[],"source":["def preprocess_validation_examples(examples):\n","    questions = [q.strip() for q in examples[\"question\"]]\n","    inputs = tokenizer(\n","        questions,\n","        examples[\"context\"],\n","        max_length=max_length,\n","        truncation=\"only_second\",\n","        stride=stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n","    example_ids = []\n","\n","    for i in range(len(inputs[\"input_ids\"])):\n","        sample_idx = sample_map[i]\n","        example_ids.append(examples[\"id\"][sample_idx])\n","\n","        sequence_ids = inputs.sequence_ids(i)\n","        offset = inputs[\"offset_mapping\"][i]\n","        inputs[\"offset_mapping\"][i] = [\n","            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n","        ]\n","\n","    inputs[\"example_id\"] = example_ids\n","    return inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"troR6GkaV-bH","outputId":"64910c9c-b821-421b-e97a-6bb7337df121"},"outputs":[{"data":{"text/plain":["(10570, 10822)"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["validation_dataset = raw_datasets[\"validation\"].map(\n","    preprocess_validation_examples,\n","    batched=True,\n","    remove_columns=raw_datasets[\"validation\"].column_names,\n",")\n","len(raw_datasets[\"validation\"]), len(validation_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XUeAma3kV-bH"},"outputs":[],"source":["small_eval_set = raw_datasets[\"validation\"].select(range(100))\n","trained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n","eval_set = small_eval_set.map(\n","    preprocess_validation_examples,\n","    batched=True,\n","    remove_columns=raw_datasets[\"validation\"].column_names,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lPXu5inqV-bI"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E1mLvfxFV-bI"},"outputs":[],"source":["import tensorflow as tf\n","from transformers import TFAutoModelForQuestionAnswering\n","\n","eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\n","eval_set_for_model.set_format(\"numpy\")\n","\n","batch = {k: eval_set_for_model[k] for k in eval_set_for_model.column_names}\n","trained_model = TFAutoModelForQuestionAnswering.from_pretrained(trained_checkpoint)\n","\n","outputs = trained_model(**batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ybfozr71V-bI"},"outputs":[],"source":["start_logits = outputs.start_logits.numpy()\n","end_logits = outputs.end_logits.numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RVXI8u_HV-bI"},"outputs":[],"source":["import collections\n","\n","example_to_features = collections.defaultdict(list)\n","for idx, feature in enumerate(eval_set):\n","    example_to_features[feature[\"example_id\"]].append(idx)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ISljigxPV-bI"},"outputs":[],"source":["import numpy as np\n","\n","n_best = 20\n","max_answer_length = 30\n","predicted_answers = []\n","\n","for example in small_eval_set:\n","    example_id = example[\"id\"]\n","    context = example[\"context\"]\n","    answers = []\n","\n","    for feature_index in example_to_features[example_id]:\n","        start_logit = start_logits[feature_index]\n","        end_logit = end_logits[feature_index]\n","        offsets = eval_set[\"offset_mapping\"][feature_index]\n","\n","        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n","        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n","        for start_index in start_indexes:\n","            for end_index in end_indexes:\n","                # Skip answers that are not fully in the context\n","                if offsets[start_index] is None or offsets[end_index] is None:\n","                    continue\n","                # Skip answers with a length that is either < 0 or > max_answer_length.\n","                if (\n","                    end_index < start_index\n","                    or end_index - start_index + 1 > max_answer_length\n","                ):\n","                    continue\n","\n","                answers.append(\n","                    {\n","                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n","                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n","                    }\n","                )\n","\n","    best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n","    predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cmrvoKdjV-bI"},"outputs":[],"source":["import evaluate\n","\n","metric = evaluate.load(\"squad\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v65X4DzDV-bI"},"outputs":[],"source":["theoretical_answers = [\n","    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A7R1xJhiV-bJ","outputId":"a510a41f-19a8-4738-a186-26e39286bb35"},"outputs":[{"data":{"text/plain":["{'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'}\n","{'id': '56be4db0acb8001400a502ec', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["print(predicted_answers[0])\n","print(theoretical_answers[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vrnWoxPZV-bJ","outputId":"a08d3da0-db8d-4942-8799-2c6f7fb9ba51"},"outputs":[{"data":{"text/plain":["{'exact_match': 83.0, 'f1': 88.25}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["metric.compute(predictions=predicted_answers, references=theoretical_answers)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WNYBBH6FV-bJ"},"outputs":[],"source":["from tqdm.auto import tqdm\n","\n","\n","def compute_metrics(start_logits, end_logits, features, examples):\n","    example_to_features = collections.defaultdict(list)\n","    for idx, feature in enumerate(features):\n","        example_to_features[feature[\"example_id\"]].append(idx)\n","\n","    predicted_answers = []\n","    for example in tqdm(examples):\n","        example_id = example[\"id\"]\n","        context = example[\"context\"]\n","        answers = []\n","\n","        # Loop through all features associated with that example\n","        for feature_index in example_to_features[example_id]:\n","            start_logit = start_logits[feature_index]\n","            end_logit = end_logits[feature_index]\n","            offsets = features[feature_index][\"offset_mapping\"]\n","\n","            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n","            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n","            for start_index in start_indexes:\n","                for end_index in end_indexes:\n","                    # Skip answers that are not fully in the context\n","                    if offsets[start_index] is None or offsets[end_index] is None:\n","                        continue\n","                    # Skip answers with a length that is either < 0 or > max_answer_length\n","                    if (\n","                        end_index < start_index\n","                        or end_index - start_index + 1 > max_answer_length\n","                    ):\n","                        continue\n","\n","                    answer = {\n","                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n","                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n","                    }\n","                    answers.append(answer)\n","\n","        # Select the answer with the best score\n","        if len(answers) > 0:\n","            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n","            predicted_answers.append(\n","                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n","            )\n","        else:\n","            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n","\n","    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n","    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FEQ8xbllV-bJ","outputId":"3ec3361e-b798-4a9f-8ddc-a6a4e970f2fa"},"outputs":[{"data":{"text/plain":["{'exact_match': 83.0, 'f1': 88.25}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["compute_metrics(start_logits, end_logits, eval_set, small_eval_set)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"98yX04qUV-bS"},"outputs":[],"source":["model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6cTMwMsrV-bS"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DWhjanFAV-bS"},"outputs":[],"source":["from transformers import DefaultDataCollator\n","\n","data_collator = DefaultDataCollator(return_tensors=\"tf\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NvVE76m2V-bS"},"outputs":[],"source":["tf_train_dataset = model.prepare_tf_dataset(\n","    train_dataset,\n","    collate_fn=data_collator,\n","    shuffle=True,\n","    batch_size=16,\n",")\n","tf_eval_dataset = model.prepare_tf_dataset(\n","    validation_dataset,\n","    collate_fn=data_collator,\n","    shuffle=False,\n","    batch_size=16,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DSzBDsKbV-bT"},"outputs":[],"source":["from transformers import create_optimizer\n","from transformers.keras_callbacks import PushToHubCallback\n","import tensorflow as tf\n","\n","# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n","# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n","# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n","num_train_epochs = 3\n","num_train_steps = len(tf_train_dataset) * num_train_epochs\n","optimizer, schedule = create_optimizer(\n","    init_lr=2e-5,\n","    num_warmup_steps=0,\n","    num_train_steps=num_train_steps,\n","    weight_decay_rate=0.01,\n",")\n","model.compile(optimizer=optimizer)\n","\n","# Train in mixed-precision float16\n","tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6I5ytroTV-bT"},"outputs":[],"source":["from transformers.keras_callbacks import PushToHubCallback\n","\n","callback = PushToHubCallback(output_dir=\"bert-finetuned-squad\", tokenizer=tokenizer)\n","\n","# We're going to do validation afterwards, so no validation mid-training\n","model.fit(tf_train_dataset, callbacks=[callback], epochs=num_train_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7arvFKxFV-bT","outputId":"86ecac37-dac6-4508-a6cd-dc28c24d564f"},"outputs":[{"data":{"text/plain":["{'exact_match': 81.18259224219489, 'f1': 88.67381321905516}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["predictions = model.predict(tf_eval_dataset)\n","compute_metrics(\n","    predictions[\"start_logits\"],\n","    predictions[\"end_logits\"],\n","    validation_dataset,\n","    raw_datasets[\"validation\"],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2uOVjGYBV-bT","outputId":"23e09668-05ad-4c67-c6d5-14ecf400747d"},"outputs":[{"data":{"text/plain":["{'score': 0.9979003071784973,\n"," 'start': 78,\n"," 'end': 105,\n"," 'answer': 'Jax, PyTorch and TensorFlow'}"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import pipeline\n","\n","# Replace this with your own checkpoint\n","model_checkpoint = \"huggingface-course/bert-finetuned-squad\"\n","question_answerer = pipeline(\"question-answering\", model=model_checkpoint)\n","\n","context = \"\"\"\n","🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration\n","between them. It's straightforward to train your models with one before loading them for inference with the other.\n","\"\"\"\n","question = \"Which deep learning libraries back 🤗 Transformers?\"\n","question_answerer(question=question, context=context)"]}]}