{"cells":[{"cell_type":"markdown","source":["# 2.2 Behind The Pipeline"],"metadata":{"id":"hDagCtPjGs1C"}},{"cell_type":"code","source":["# Importing necessary libraries\n","import tensorflow as tf\n","from transformers import pipeline, AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification, BertConfig, TFBertModel, BertTokenizer"],"metadata":{"id":"8CMxu1S5FOYb","executionInfo":{"status":"ok","timestamp":1706496628731,"user_tz":-420,"elapsed":2,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","execution_count":20,"metadata":{"id":"WH3NTz47oT7y","outputId":"4619cc15-e12a-489e-9381-46e4771d6a73","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706496636574,"user_tz":-420,"elapsed":1435,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'label': 'POSITIVE', 'score': 0.9997958540916443},\n"," {'label': 'NEGATIVE', 'score': 0.9514424204826355}]"]},"metadata":{},"execution_count":20}],"source":["# Multi sentences sentiment-analysis\n","classifier = pipeline(\"sentiment-analysis\")\n","classifier(\n","    [\n","        \"I love doing ice skating!\",\n","        \"But, I hate cold season\",\n","    ]\n",")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"8OAeibNwoT7z","executionInfo":{"status":"ok","timestamp":1706496023618,"user_tz":-420,"elapsed":319,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}}},"outputs":[],"source":["checkpoint  = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer   = AutoTokenizer.from_pretrained(checkpoint)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"u0eObXtNoT7z","outputId":"7d7b697e-35f7-4d6f-d936-a604d5049083","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706496149443,"user_tz":-420,"elapsed":516,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': <tf.Tensor: shape=(2, 8), dtype=int32, numpy=\n","array([[  101,  1045,  2293,  2725,  3256, 10080,   999,   102],\n","       [  101,  2021,  1010,  1045,  5223,  3147,  2161,   102]],\n","      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 8), dtype=int32, numpy=\n","array([[1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n"]}],"source":["raw_inputs = [\n","    \"I love doing ice skating!\",\n","    \"But, I hate cold season\",\n","]\n","\n","inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")\n","print(inputs)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"PT4FdtRQoT70","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706496154824,"user_tz":-420,"elapsed":5060,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}},"outputId":"f0a59f86-373c-4c20-c6ba-d0c950bd5a13"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n","- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFDistilBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["(2, 8, 768)\n"]}],"source":["checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","model = TFAutoModel.from_pretrained(checkpoint)\n","\n","outputs = model(inputs)\n","print(outputs.last_hidden_state.shape)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"N1nHtVxqoT71","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706496162251,"user_tz":-420,"elapsed":3813,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}},"outputId":"a2d29c8f-b907-4ed6-b970-942d3da19f93"},"outputs":[{"output_type":"stream","name":"stderr","text":["All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n","\n","All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["(2, 2)\n","tf.Tensor(\n","[[-4.0918717  4.404614 ]\n"," [ 1.5754856 -1.399742 ]], shape=(2, 2), dtype=float32)\n","tf.Tensor(\n","[[2.0414300e-04 9.9979585e-01]\n"," [9.5144242e-01 4.8557643e-02]], shape=(2, 2), dtype=float32)\n"]}],"source":["checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","outputs = model(inputs)\n","print(outputs.logits.shape)\n","print(outputs.logits)\n","\n","predictions = tf.math.softmax(outputs.logits, axis=-1)\n","print(predictions)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"al0Sf6YYoT73","outputId":"97bdc49d-b818-4fc6-dc90-98919374c323","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706496192949,"user_tz":-420,"elapsed":312,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'NEGATIVE', 1: 'POSITIVE'}"]},"metadata":{},"execution_count":13}],"source":["model.config.id2label"]},{"cell_type":"markdown","source":["# 2.3 Models"],"metadata":{"id":"JYTbP1HDG7aZ"}},{"cell_type":"code","source":["# Building the config\n","config = BertConfig()\n","\n","# Building the model from the config\n","model = TFBertModel(config)\n","print(config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iU4MT_j0HEDS","executionInfo":{"status":"ok","timestamp":1706496393840,"user_tz":-420,"elapsed":2025,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}},"outputId":"67ee1b7e-2af8-4aef-edc0-13d2e344f0f4"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.35.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n"]}]},{"cell_type":"code","source":["# Initialize the model randomly\n","config = BertConfig()\n","model = TFBertModel(config)\n","model = TFBertModel.from_pretrained(\"bert-base-cased\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":192,"referenced_widgets":["e796fa8bc87d4822a4d4d9bbe328bbde","255a38a7f5ac41f99f4e34235b6a3050","252dd9c130004b96adcf349f6641d8a0","8c97b13994ca4e2aaba4f9d91482d49c","54b9232d0e6b478ea6374b6b81047359","cbf87cb18dd14c15b267452428b0c701","a8297a0ee34a431bb5db7285012d9795","1123866c04ce418295340335d1f4137d","34d7328d84414827b8effe04808fb892","e52175d91982473b87adff1d620f94e2","b020817ffc124cd78e830e7ec2809548","c7e1627f48e9418d8c66e2fe39208b20","3df2c06ebbd44054bbfb9db6f923f238","d0c0822afcfd442aa6e02798f3e16f72","4fb766ef76f64cf0b71dd4699d5e7309","f02a4142d05c47fbae08f9a8f2472b30","c98a0723de554fd6bf07de056e07de7a","175b6b27ab234fe0942e6c9caae2eaf9","3a3bbf513e2d49f5b266bbef95acc5c4","f25674c506034ed98ef73408eae239da","69ab2d2dfa3840268dbd65d5bda7c86c","e2966f78e9244a5ba27d98bb8e2620cc"]},"id":"s4HEGnbYHHhx","executionInfo":{"status":"ok","timestamp":1706496450188,"user_tz":-420,"elapsed":13714,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}},"outputId":"a39213cc-6e92-4bf6-8900-4396878b2e11"},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e796fa8bc87d4822a4d4d9bbe328bbde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7e1627f48e9418d8c66e2fe39208b20"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]}]},{"cell_type":"code","source":["model.save_pretrained(\"pretrained_model\")"],"metadata":{"id":"862FENYhHSVx","executionInfo":{"status":"ok","timestamp":1706496504860,"user_tz":-420,"elapsed":5544,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]\n","encoded_sequences = [\n","    [101, 7592, 999, 102],\n","    [101, 4658, 1012, 102],\n","    [101, 3835, 999, 102],\n","]\n","model_inputs = tf.constant(encoded_sequences)\n","output = model(model_inputs)\n","\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YUWZrFSHHhsZ","executionInfo":{"status":"ok","timestamp":1706496539146,"user_tz":-420,"elapsed":3,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}},"outputId":"1c59af0c-9599-4e45-e568-698a23d751fe"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor: shape=(3, 4, 768), dtype=float32, numpy=\n","array([[[ 4.4495744e-01,  4.8276284e-01,  2.7797198e-01, ...,\n","         -5.4032274e-02,  3.9393425e-01, -9.4769992e-02],\n","        [ 2.4942899e-01, -4.4092962e-01,  8.1772339e-01, ...,\n","         -3.1916568e-01,  2.2992219e-01, -4.1171715e-02],\n","        [ 1.3667546e-01,  2.2517854e-01,  1.4501992e-01, ...,\n","         -4.6914592e-02,  2.8224233e-01,  7.5565845e-02],\n","        [ 1.1788858e+00,  1.6738570e-01, -1.8187097e-01, ...,\n","          2.4671350e-01,  1.0440764e+00, -6.1971312e-03]],\n","\n","       [[ 3.6435828e-01,  3.2463297e-02,  2.0257683e-01, ...,\n","          6.0110353e-02,  3.2451293e-01, -2.0995583e-02],\n","        [ 7.1865958e-01, -4.8725206e-01,  5.1740402e-01, ...,\n","         -4.4012007e-01,  1.4553043e-01, -3.7544727e-02],\n","        [ 3.3223230e-01, -2.3270901e-01,  9.4876081e-02, ...,\n","         -2.5268167e-01,  3.2172003e-01,  8.1142318e-04],\n","        [ 1.2523208e+00,  3.5754293e-01, -5.1321149e-02, ...,\n","         -3.7839708e-01,  1.0526475e+00, -5.6254762e-01]],\n","\n","       [[ 2.4042264e-01,  1.4717777e-01,  1.2110285e-01, ...,\n","          7.6061681e-02,  3.3564472e-01,  2.8261665e-01],\n","        [ 6.5700614e-01, -3.2786581e-01,  2.4967620e-01, ...,\n","         -2.5919506e-01,  2.0174681e-01,  3.3275127e-01],\n","        [ 2.0159575e-01,  1.5782663e-01,  9.8975692e-03, ...,\n","         -3.8850459e-01,  4.1307470e-01,  3.9731947e-01],\n","        [ 1.0174985e+00,  6.4386743e-01, -7.8146642e-01, ...,\n","         -4.2109150e-01,  1.0925050e+00, -4.8456412e-02]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(3, 768), dtype=float32, numpy=\n","array([[-0.6856277 ,  0.5262493 ,  0.99995273, ...,  0.99998754,\n","        -0.61123455,  0.9970657 ],\n","       [-0.60545814,  0.49971724,  0.99981916, ...,  0.9999408 ,\n","        -0.67532754,  0.97692657],\n","       [-0.77015084,  0.54474187,  0.9999417 , ...,  0.9999845 ,\n","        -0.4654935 ,  0.98939013]], dtype=float32)>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n"]}]},{"cell_type":"markdown","source":["# 2.4 Tokenizer"],"metadata":{"id":"rQpxhiONIGhC"}},{"cell_type":"code","source":["tokenized_text = \"Elon Musk is a CEO\".split()\n","print(tokenized_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uSt8jgnLILwj","executionInfo":{"status":"ok","timestamp":1706496738607,"user_tz":-420,"elapsed":370,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}},"outputId":"2281828b-954c-41da-fe33-841df5f36fcc"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["['Elon', 'Musk', 'is', 'a', 'CEO']\n"]}]},{"cell_type":"code","source":["tokenizer_bert = BertTokenizer.from_pretrained(\"bert-base-cased\")\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"],"metadata":{"id":"pl27Re7EIVsA","executionInfo":{"status":"ok","timestamp":1706496788478,"user_tz":-420,"elapsed":691,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["tokenizer(\"Using a Transformer network is simple\")\n","tokenizer.save_pretrained(\"tokenizer\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YCkbXxahIflx","executionInfo":{"status":"ok","timestamp":1706496809410,"user_tz":-420,"elapsed":338,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}},"outputId":"8bc62027-9099-42cd-ec68-80d02db4371d"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('tokenizer/tokenizer_config.json',\n"," 'tokenizer/special_tokens_map.json',\n"," 'tokenizer/vocab.txt',\n"," 'tokenizer/added_tokens.json',\n"," 'tokenizer/tokenizer.json')"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","\n","sequence = \"Using a Transformer network is simple\"\n","tokens = tokenizer.tokenize(sequence)\n","\n","print(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UHdEEeiCIqFZ","executionInfo":{"status":"ok","timestamp":1706496827481,"user_tz":-420,"elapsed":3,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}},"outputId":"76d1be21-b167-46b2-ef6c-e4b5eb8657a2"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"]}]},{"cell_type":"code","source":["ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","print(ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QhF27VPwIxvQ","executionInfo":{"status":"ok","timestamp":1706496835266,"user_tz":-420,"elapsed":328,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}},"outputId":"204d414e-2e8a-405e-e62f-df4ae3d4dc53"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"]}]},{"cell_type":"code","source":["decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n","print(decoded_string)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nQBA0Z16IzrY","executionInfo":{"status":"ok","timestamp":1706496840599,"user_tz":-420,"elapsed":331,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}},"outputId":"e534d1c2-fb63-4347-88c8-46522e4b26ea"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Using a transformer network is simple\n"]}]},{"cell_type":"markdown","source":["# 2.5 Handling Multiple Sequences"],"metadata":{"id":"tf6PPMVgI2G_"}},{"cell_type":"code","source":["checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","tokens = tokenizer.tokenize(sequence)\n","ids = tokenizer.convert_tokens_to_ids(tokens)\n","input_ids = tf.constant(ids)\n","model(input_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3UdIvVFSI6Tp","executionInfo":{"status":"ok","timestamp":1706496894929,"user_tz":-420,"elapsed":5733,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}},"outputId":"68d9a0e4-14a3-448d-cf08-5698780dcce1"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n","\n","All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"]},{"output_type":"execute_result","data":{"text/plain":["TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-2.7276218,  2.8789387]], dtype=float32)>, hidden_states=None, attentions=None)"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["tokenized_inputs = tokenizer(sequence, return_tensors=\"tf\")\n","print(tokenized_inputs[\"input_ids\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EFPei6_6JA3Z","executionInfo":{"status":"ok","timestamp":1706496898887,"user_tz":-420,"elapsed":321,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}},"outputId":"7b3e431d-8fee-444f-93fd-e56db188bfff"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[  101  1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026\n","   2878  2166  1012   102]], shape=(1, 16), dtype=int32)\n"]}]},{"cell_type":"code","source":["checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","tokens = tokenizer.tokenize(sequence)\n","ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","input_ids = tf.constant([ids])\n","print(\"Input IDs:\", input_ids)\n","\n","output = model(input_ids)\n","print(\"Logits:\", output.logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g2GJtnzoJDKR","executionInfo":{"status":"ok","timestamp":1706496979485,"user_tz":-420,"elapsed":3158,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}},"outputId":"05bb325a-5868-4f8f-be3a-4e95e9c808c8"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stderr","text":["All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n","\n","All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Input IDs: tf.Tensor(\n","[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878\n","   2166  1012]], shape=(1, 14), dtype=int32)\n","Logits: tf.Tensor([[-2.7276218  2.8789387]], shape=(1, 2), dtype=float32)\n"]}]},{"cell_type":"code","source":["batched_ids = [\n","    [200, 200, 200],\n","    [200, 200]\n","]\n","\n","padding_id = 100\n","\n","batched_ids = [\n","    [200, 200, 200],\n","    [200, 200, padding_id],\n","]"],"metadata":{"id":"FSniu59-JEzg","executionInfo":{"status":"ok","timestamp":1706496959137,"user_tz":-420,"elapsed":3,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequence1_ids = [[200, 200, 200]]\n","sequence2_ids = [[200, 200]]\n","batched_ids = [\n","    [200, 200, 200],\n","    [200, 200, tokenizer.pad_token_id],\n","]\n","\n","print(model(tf.constant(sequence1_ids)).logits)\n","print(model(tf.constant(sequence2_ids)).logits)\n","print(model(tf.constant(batched_ids)).logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hLH_wusGJHMJ","executionInfo":{"status":"ok","timestamp":1706496964617,"user_tz":-420,"elapsed":5483,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}},"outputId":"24a8551f-15c7-4547-ad0c-e1b6174e24db"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stderr","text":["All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n","\n","All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["tf.Tensor([[ 1.5693682 -1.3894583]], shape=(1, 2), dtype=float32)\n","tf.Tensor([[ 0.5803017  -0.41252568]], shape=(1, 2), dtype=float32)\n","tf.Tensor(\n","[[ 1.569368  -1.3894587]\n"," [ 1.33735   -1.2163203]], shape=(2, 2), dtype=float32)\n"]}]},{"cell_type":"code","source":["batched_ids = [\n","    [200, 200, 200],\n","    [200, 200, tokenizer.pad_token_id],\n","]\n","\n","attention_mask = [\n","    [1, 1, 1],\n","    [1, 1, 0],\n","]\n","\n","outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))\n","print(outputs.logits)\n","\n","max_sequence_length = 100\n","sequence = sequence[:max_sequence_length]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aJr_KcLQJIpo","executionInfo":{"status":"ok","timestamp":1706497014830,"user_tz":-420,"elapsed":349,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}},"outputId":"f3d60a86-4ba8-4d09-d6ee-b79a8b06e7fe"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[ 1.569368   -1.3894587 ]\n"," [ 0.58030075 -0.4125247 ]], shape=(2, 2), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["# 2.6 Putting It All Together"],"metadata":{"id":"fl07Bm-pJlpJ"}},{"cell_type":"code","source":["checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","model_inputs = tokenizer(sequence)"],"metadata":{"id":"L9EZ_W9aJssI","executionInfo":{"status":"ok","timestamp":1706497071705,"user_tz":-420,"elapsed":893,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","model_inputs = tokenizer(sequence)"],"metadata":{"id":"s571vBehJtNo","executionInfo":{"status":"ok","timestamp":1706497080755,"user_tz":-420,"elapsed":341,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n","\n","model_inputs = tokenizer(sequences)"],"metadata":{"id":"f4uRNq0fJvjw","executionInfo":{"status":"ok","timestamp":1706497081471,"user_tz":-420,"elapsed":1,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["# Will pad the sequences up to the maximum sequence length\n","model_inputs = tokenizer(sequences, padding=\"longest\")\n","\n","# Will pad the sequences up to the model max length\n","# (512 for BERT or DistilBERT)\n","model_inputs = tokenizer(sequences, padding=\"max_length\")\n","\n","# Will pad the sequences up to the specified max length\n","model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"],"metadata":{"id":"QbyISDLdJv3Y","executionInfo":{"status":"ok","timestamp":1706497091342,"user_tz":-420,"elapsed":508,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n","\n","# Will truncate the sequences that are longer than the model max length\n","# (512 for BERT or DistilBERT)\n","model_inputs = tokenizer(sequences, truncation=True)\n","\n","# Will truncate the sequences that are longer than the specified max length\n","model_inputs = tokenizer(sequences, max_length=8, truncation=True)"],"metadata":{"id":"-WAsU1dWJyLX","executionInfo":{"status":"ok","timestamp":1706497092243,"user_tz":-420,"elapsed":1,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n","\n","# Returns PyTorch tensors\n","model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n","\n","# Returns TensorFlow tensors\n","model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n","\n","# Returns NumPy arrays\n","model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")"],"metadata":{"id":"COWbUD6FJydA","executionInfo":{"status":"ok","timestamp":1706497097923,"user_tz":-420,"elapsed":340,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","model_inputs = tokenizer(sequence)\n","print(model_inputs[\"input_ids\"])\n","\n","tokens = tokenizer.tokenize(sequence)\n","ids = tokenizer.convert_tokens_to_ids(tokens)\n","print(ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gBVG86g0Jzyf","executionInfo":{"status":"ok","timestamp":1706497102329,"user_tz":-420,"elapsed":324,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}},"outputId":"8c18dd48-6bb4-4a47-ecd5-04c707b62a8a"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n","[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n"]}]},{"cell_type":"code","source":["print(tokenizer.decode(model_inputs[\"input_ids\"]))\n","print(tokenizer.decode(ids))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"roWcgfZhJ02A","executionInfo":{"status":"ok","timestamp":1706497111203,"user_tz":-420,"elapsed":2,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}},"outputId":"dfc9eafc-ed0a-42f6-9487-79a6bb4d0488"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS] i've been waiting for a huggingface course my whole life. [SEP]\n","i've been waiting for a huggingface course my whole life.\n"]}]},{"cell_type":"code","source":["checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n","sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n","\n","tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\")\n","output = model(**tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CMHrZ1pLJ3Cv","executionInfo":{"status":"ok","timestamp":1706497118498,"user_tz":-420,"elapsed":6304,"user":{"displayName":"Miftahul Tirta Irawan","userId":"03385890976200998732"}},"outputId":"2696db0f-def1-4130-ec25-df4371a54f34"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stderr","text":["All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n","\n","All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/huggingface/notebooks/blob/master/course/en/chapter2/section2_tf.ipynb","timestamp":1687932744525}]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e796fa8bc87d4822a4d4d9bbe328bbde":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_255a38a7f5ac41f99f4e34235b6a3050","IPY_MODEL_252dd9c130004b96adcf349f6641d8a0","IPY_MODEL_8c97b13994ca4e2aaba4f9d91482d49c"],"layout":"IPY_MODEL_54b9232d0e6b478ea6374b6b81047359"}},"255a38a7f5ac41f99f4e34235b6a3050":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbf87cb18dd14c15b267452428b0c701","placeholder":"​","style":"IPY_MODEL_a8297a0ee34a431bb5db7285012d9795","value":"config.json: 100%"}},"252dd9c130004b96adcf349f6641d8a0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1123866c04ce418295340335d1f4137d","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_34d7328d84414827b8effe04808fb892","value":570}},"8c97b13994ca4e2aaba4f9d91482d49c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e52175d91982473b87adff1d620f94e2","placeholder":"​","style":"IPY_MODEL_b020817ffc124cd78e830e7ec2809548","value":" 570/570 [00:00&lt;00:00, 11.9kB/s]"}},"54b9232d0e6b478ea6374b6b81047359":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbf87cb18dd14c15b267452428b0c701":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8297a0ee34a431bb5db7285012d9795":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1123866c04ce418295340335d1f4137d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34d7328d84414827b8effe04808fb892":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e52175d91982473b87adff1d620f94e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b020817ffc124cd78e830e7ec2809548":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c7e1627f48e9418d8c66e2fe39208b20":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3df2c06ebbd44054bbfb9db6f923f238","IPY_MODEL_d0c0822afcfd442aa6e02798f3e16f72","IPY_MODEL_4fb766ef76f64cf0b71dd4699d5e7309"],"layout":"IPY_MODEL_f02a4142d05c47fbae08f9a8f2472b30"}},"3df2c06ebbd44054bbfb9db6f923f238":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c98a0723de554fd6bf07de056e07de7a","placeholder":"​","style":"IPY_MODEL_175b6b27ab234fe0942e6c9caae2eaf9","value":"model.safetensors: 100%"}},"d0c0822afcfd442aa6e02798f3e16f72":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a3bbf513e2d49f5b266bbef95acc5c4","max":435755784,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f25674c506034ed98ef73408eae239da","value":435755784}},"4fb766ef76f64cf0b71dd4699d5e7309":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69ab2d2dfa3840268dbd65d5bda7c86c","placeholder":"​","style":"IPY_MODEL_e2966f78e9244a5ba27d98bb8e2620cc","value":" 436M/436M [00:06&lt;00:00, 91.4MB/s]"}},"f02a4142d05c47fbae08f9a8f2472b30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c98a0723de554fd6bf07de056e07de7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"175b6b27ab234fe0942e6c9caae2eaf9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a3bbf513e2d49f5b266bbef95acc5c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f25674c506034ed98ef73408eae239da":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"69ab2d2dfa3840268dbd65d5bda7c86c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2966f78e9244a5ba27d98bb8e2620cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}